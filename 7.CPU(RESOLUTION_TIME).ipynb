{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a19faad-a020-4ed6-b47a-58b9d00921e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "012942e9-7ba6-4d20-b130-59842df85812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NLP_Dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d10a05-f877-40b3-b346-691ba5f6fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3388eaea-896a-4873-ae47-05910460b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be48824a-dad6-459b-9fa4-692ae3b32143",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e991c56-0aff-47ab-95de-a8e62b51e7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2292/3828475250.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/3828475250.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/3828475250.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/3828475250.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n"
     ]
    }
   ],
   "source": [
    "selected_columns = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'ActivatedTimestamp', 'ClearedTimestamp', 'month', 'week', 'ResolutionTimeMinutes'\n",
    "]\n",
    "df_selected = df_sample[selected_columns]\n",
    "\n",
    "# Convert categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity']:\n",
    "    le = LabelEncoder()\n",
    "    df_selected[column] = le.fit_transform(df_selected[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Define features and target\n",
    "X = df_selected.drop(['ResolutionTimeMinutes', 'ActivatedTimestamp', 'ClearedTimestamp'], axis=1)\n",
    "y = df_selected['ResolutionTimeMinutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c351ca2c-de56-422b-b5ca-b949ed40ed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(X, y)\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# Mutual Information for feature importance\n",
    "mi = mutual_info_regression(X, y)\n",
    "mi_importances = mi\n",
    "\n",
    "# Plot feature importances\n",
    "feature_names = X.columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "axs[0].barh(feature_names, rf_importances)\n",
    "axs[0].set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Mutual Information Feature Importance\n",
    "axs[1].barh(feature_names, mi_importances)\n",
    "axs[1].set_title('Mutual Information Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb6834ef-24e4-4b1f-be98-a6032b4a7f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Validation MSE: 5.5366006158348386e-05, R²: -0.3316400381472777\n",
      "Neural Network - Validation MSE: 0.5054521697518128, R²: -12155.924317150475\n",
      "Random Forest - Test MSE: 0.0002382707199409722, R²: -0.08333139853105354\n",
      "Neural Network - Test MSE: 0.5013997457898863, R²: -2278.684587196602\n",
      "Original Scale Predictions - Random Forest: [5.97385812 4.26698154 1.1023522  0.19973791 3.44292817]\n",
      "Original Scale Predictions - Neural Network: [173444.17022788  10997.14443658 116292.83862791 262498.16627747\n",
      "  96899.478182  ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Remove less important columns\n",
    "important_features = ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity', 'month', 'week']\n",
    "X = df_selected[important_features]\n",
    "\n",
    "# Normalize the ResolutionTimeMinutes column\n",
    "scaler = MinMaxScaler()\n",
    "y_normalized = scaler.fit_transform(df_selected[['ResolutionTimeMinutes']])\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_normalized, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Neural Network Regressor\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500)\n",
    "nn_model.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Predict and evaluate on validation data\n",
    "rf_pred_val = rf_model.predict(X_val)\n",
    "nn_pred_val = nn_model.predict(X_val)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_mse_val = mean_squared_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Evaluate Neural Network\n",
    "nn_mse_val = mean_squared_error(y_val, nn_pred_val)\n",
    "nn_r2_val = r2_score(y_val, nn_pred_val)\n",
    "\n",
    "print(f\"Random Forest - Validation MSE: {rf_mse_val}, R²: {rf_r2_val}\")\n",
    "print(f\"Neural Network - Validation MSE: {nn_mse_val}, R²: {nn_r2_val}\")\n",
    "\n",
    "# Predict on test data\n",
    "rf_pred_test = rf_model.predict(X_test)\n",
    "nn_pred_test = nn_model.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest on test data\n",
    "rf_mse_test = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_r2_test = r2_score(y_test, rf_pred_test)\n",
    "\n",
    "# Evaluate Neural Network on test data\n",
    "nn_mse_test = mean_squared_error(y_test, nn_pred_test)\n",
    "nn_r2_test = r2_score(y_test, nn_pred_test)\n",
    "\n",
    "print(f\"Random Forest - Test MSE: {rf_mse_test}, R²: {rf_r2_test}\")\n",
    "print(f\"Neural Network - Test MSE: {nn_mse_test}, R²: {nn_r2_test}\")\n",
    "\n",
    "# Convert predicted values back to original scale\n",
    "rf_pred_test_original = scaler.inverse_transform(rf_pred_test.reshape(-1, 1))\n",
    "nn_pred_test_original = scaler.inverse_transform(nn_pred_test.reshape(-1, 1))\n",
    "\n",
    "# Display a few predicted values\n",
    "print(f\"Original Scale Predictions - Random Forest: {rf_pred_test_original[:5].ravel()}\")\n",
    "print(f\"Original Scale Predictions - Neural Network: {nn_pred_test_original[:5].ravel()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c5a1c1-4bae-474b-a489-2e472311088d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23.5\n",
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "print(np.__version__)\n",
    "print(xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0368c17-9a33-45e3-9d7c-24cd83c0cb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2292/1988221402.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/1988221402.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/1988221402.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/1988221402.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/1988221402.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
      "/tmp/ipykernel_2292/1988221402.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
      "/tmp/ipykernel_2292/1988221402.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/1988221402.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/1988221402.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/1988221402.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/1988221402.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Validation MSE: 0.0014553117983205947, R²: 0.6578449865910176\n",
      "Neural Network - Validation MSE: 0.0012177967371169642, R²: 0.713686606953571\n",
      "Random Forest - Test MSE: 0.0013751059245046735, R²: 0.6753424359110427\n",
      "Neural Network - Test MSE: 0.0013445327097768823, R²: 0.6825606619713143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'ActivatedTimestamp', 'ClearedTimestamp', 'month', 'week', 'ResolutionTimeMinutes'\n",
    "]\n",
    "df_selected = df_sample[selected_columns]\n",
    "\n",
    "# Convert categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity']:\n",
    "    le = LabelEncoder()\n",
    "    df_selected[column] = le.fit_transform(df_selected[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Convert timestamps to datetime and extract features\n",
    "df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
    "df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
    "df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
    "df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
    "df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
    "df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
    "df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n",
    "\n",
    "# Remove rows with invalid resolution times\n",
    "df_selected = df_selected[(df_selected['ResolutionTime'] >= 0) & (df_selected['ResolutionTime'] <= 10000)]\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = np.abs(stats.zscore(df_selected['ResolutionTimeMinutes']))\n",
    "df_selected = df_selected[z_scores < 3]\n",
    "\n",
    "# Normalize the ResolutionTimeMinutes column\n",
    "scaler = MinMaxScaler()\n",
    "df_selected['ResolutionTimeMinutes'] = scaler.fit_transform(df_selected[['ResolutionTimeMinutes']])\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'month', 'week', 'ActivationHour', 'ClearanceHour',\n",
    "    'ActivationDayOfWeek', 'ClearanceDayOfWeek'\n",
    "]\n",
    "X = df_selected[features]\n",
    "y = df_selected['ResolutionTimeMinutes']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# Mutual Information for feature importance\n",
    "mi = mutual_info_regression(X_train, y_train)\n",
    "mi_importances = mi\n",
    "\n",
    "# Plot feature importances\n",
    "feature_names = X_train.columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "axs[0].barh(feature_names, rf_importances)\n",
    "axs[0].set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Mutual Information Feature Importance\n",
    "axs[1].barh(feature_names, mi_importances)\n",
    "axs[1].set_title('Mutual Information Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove less important columns\n",
    "important_features = ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity', 'month', 'week', 'ActivationHour', 'ClearanceHour']\n",
    "X_train = X_train[important_features]\n",
    "X_val = X_val[important_features]\n",
    "X_test = X_test[important_features]\n",
    "\n",
    "# Model tuning and training\n",
    "# Random Forest Regressor with GridSearchCV\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_param_grid, cv=3, n_jobs=-1)\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Neural Network Regressor with GridSearchCV\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'max_iter': [300, 500]\n",
    "}\n",
    "nn_grid = GridSearchCV(MLPRegressor(random_state=42), nn_param_grid, cv=3, n_jobs=-1)\n",
    "nn_grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on validation data\n",
    "rf_pred_val = rf_grid.best_estimator_.predict(X_val)\n",
    "nn_pred_val = nn_grid.best_estimator_.predict(X_val)\n",
    "\n",
    "# Evaluate Random Forest\n",
    "rf_mse_val = mean_squared_error(y_val, rf_pred_val)\n",
    "rf_r2_val = r2_score(y_val, rf_pred_val)\n",
    "\n",
    "# Evaluate Neural Network\n",
    "nn_mse_val = mean_squared_error(y_val, nn_pred_val)\n",
    "nn_r2_val = r2_score(y_val, nn_pred_val)\n",
    "\n",
    "print(f\"Random Forest - Validation MSE: {rf_mse_val}, R²: {rf_r2_val}\")\n",
    "print(f\"Neural Network - Validation MSE: {nn_mse_val}, R²: {nn_r2_val}\")\n",
    "\n",
    "# Predict on test data\n",
    "rf_pred_test = rf_grid.best_estimator_.predict(X_test)\n",
    "nn_pred_test = nn_grid.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest on test data\n",
    "rf_mse_test = mean_squared_error(y_test, rf_pred_test)\n",
    "rf_r2_test = r2_score(y_test, rf_pred_test)\n",
    "\n",
    "# Evaluate Neural Network on test data\n",
    "nn_mse_test = mean_squared_error(y_test, nn_pred_test)\n",
    "nn_r2_test = r2_score(y_test, nn_pred_test)\n",
    "\n",
    "print(f\"Random Forest - Test MSE: {rf_mse_test}, R²: {rf_r2_test}\")\n",
    "print(f\"Neural Network - Test MSE: {nn_mse_test}, R²: {nn_r2_test}\")\n",
    "\n",
    "# Convert predicted values back to original scale\n",
    "rf_pred_test_original = scaler.inverse_transform(rf_pred_test.reshape(-1, 1))\n",
    "nn_pred_test_original = scaler.inverse_transform(nn_pred_test.reshape(-1, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5e6acfc-00e0-46f4-8805-8dc406e5e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "##below is more optimal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5db8195-dcf0-4167-b628-00f81c2b07d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[K     |████████████████████████████████| 380 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.local/lib/python3.9/site-packages (from optuna) (1.23.5)\n",
      "Requirement already satisfied: tqdm in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from optuna) (4.64.0)\n",
      "Requirement already satisfied: PyYAML in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from optuna) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from optuna) (21.3)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from optuna) (1.4.32)\n",
      "Collecting alembic>=1.5.0\n",
      "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Mako\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from alembic>=1.5.0->optuna) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from packaging>=20.0->optuna) (3.0.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from sqlalchemy>=1.3.0->optuna) (1.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /shared/centos7/anaconda3/2022.05/lib/python3.9/site-packages (from Mako->alembic>=1.5.0->optuna) (2.0.1)\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.5 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08b6b3bf-43c8-4ca9-b92f-a8cc163278cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2292/954963820.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/954963820.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/954963820.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/954963820.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/954963820.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
      "/tmp/ipykernel_2292/954963820.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
      "/tmp/ipykernel_2292/954963820.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/954963820.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/954963820.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/954963820.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/954963820.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAFgCAYAAAAo31N4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABCAUlEQVR4nO3deZhlZXW//ftrN4MINCqoDQFaEUQFRGhUHMGgSUQFIlGRvNJqJMYpRvklxGiCcQiJMSExIkEiRMUZNCqKKIgogtINDc2sSCOKEzIpCjKs94/9FBzKqq7T3TXs6ro/11VXnT09e+3nnDprrz1VqgpJkiRJkjSz7jfTAUiSJEmSJAt0SZIkSZJ6wQJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2ahZIckeQjMx2HJElaPUkWJakk8yehrYcmOSvJL5O8ZzLim0xJ3pzkuJmOQ5pNLNClSZJkZZLfJPlVkp8kOSHJxjMd19pIsleSu9s2jfx8fhrXP+FOTDtYcceoGP96Ldc7rQdAJnNnbTK0WB4503FI0tpqufm3STYfNX55+65bNGQ70/a92GLeZ8jZDwWuBzatqjdNYVgTavsMPxwcV1Xvqqo/m4J1LUly16jc/1+T0OY3JyvGIdfZm3y7mp87TSELdGlyPa+qNgZ2BR4P/O3MhjMprquqjQd+nre6DSSZNxWBDfjEqBj/ZYrXt0p9KbRX12yNW5ImcDVw0MhAkp2B+89cOJNqW+DSqqrVXXAd+M4/Z1Tuf+1MBjNb+3O2xr0us0CXpkBV/QT4Ml2hDkCSw5Nc1S5DuzTJAQPTliT5ZpJ/TXJjkquT/NHA9Icn+Xpb9ivA6DMBz09ySZKbkpyZ5NED01Ym+X9JLkpya5L/aZfEfam199UkD1zdbUzy6Laum9q6nz8w7YQk70/yxSS3Ansn2TLJSUl+3rbv9QPzPyHJ0iS3JPlpkn9rk85qv29qR8f3XM0YX57kstanX06y7cC0/0hybVvnsiRPa+P/EHgz8KK2zgsH+nGfgeXvOcs+cAb8FUl+AJwx0foniPuEJEe39+hXSc5O8rAkR7W2Lk/y+IH5Vyb52/a5ujHJ8Uk2HJj+yiTfS3JDks8l2XJgWiV5TZLvAt9NMtLnF7Z1vyjJA5N8ob13N7bXvzfQxplJ3t7i/GWS0zJwtirJU5N8q31Wrk2ypI3foH3mf9De92OSrCs7zZL648PASweGDwE+NDhD+x77s4Hhe86mjvO9+DtnWzNwNjTJvkkuaDnm2iRHrEngWcX+QZIT2rb8dYtrn/a9elSS69rPUUk2aPPvleSHSf4myU+A41su+1SSj7Tv7xVJdmg55Wct9mcPxPOyltd+meT7Sf68jX8A8CVgy9x7RnvLjLoiLRPvrxyWbn/l5iSfGMxlq9Fnz013hcRNLffsMjBtzH2xFscxwJ4t9pva+HE/F234Pjl0ovVPEPfqvhdnJvmnJN9p/fV/SR60Gn39N0kuAm5N8jFgG+DzGbgSscXzk9b+WUkeO9DGCUnel+SUFu+3k2w3MP2xSb6Sbt/jp0ne3Mbfb+B9+EWSTw7GLQt0aUqkK17+CPjewOirgKcBC4C3AR9JsnBg+hOBK+iK738B/idJ2rSPAsvatLfTJeSRde0AfAx4A7AF8EW6L9j1B9p+AfAsYAfgeXRJ9M2tvfsBr2c1JFkP+DxwGvAQ4HXAiUkeNTDbS4B3ApsA32rzXwhsBfw+8IYkf9Dm/Q/gP6pqU2A74JNt/NPb783a0fFzViPG/ds2/jFdv3yDrp9GnEd3AOVBdP37qSQbVtWpwLu496z844ZdJ/AM4NHAHwyx/om8EHgL3Xt0O3AOcH4b/jTwb6PmPxj4A7r+26EtS5JnAv/U2lsIXAN8fNSy+9N9/h5TVSN9/ri2/Z+g+4wcT3emZhvgN8DoSwlfAryM7vOwPnBYW/82dJ+397Z+2BVY3pb55xbrrsAj6T4bfz9x10jSajkX2DTdgeV5wIuAoW9jGud7cSK30h0U2AzYF/iLlhfWxJj7B1W1BDgR+JcW11eBvwOeRPe9+jjgCbR80DyMLu9tS3d5PHT7BR8GHghcQHeC4X5038n/CPz3wPI/A54LbEr3nf/vSXarqlvp9nsGr7q7bnAjhtxfeSHwh8DDgV2AJavTUUl2Az4I/Dnw4Bb759IOUjDOvlhVXQa8invPym+2Gqvdn5ZDh1j/RFbnvYDuM/ZyYEvgTuA/Yei+Pojus7lZVR0E/IB2JejAlYhfArany+3n033eGNXG21q836Pb7yPJJsBXgVNbbI8ETm/LvJ6uz57Rpt0IvG/I/pkbqsoff/yZhB9gJfAr4JdA0X0RbbaK+ZcD+7XXS4DvDUzbqLXxMLqC6E7gAQPTPwp8pL1+K/DJgWn3A34E7DUQ18ED008C3j8w/Drgs+PEuBdwN3DTwM8L6ZLbT4D7Dcz7MeCI9voE4EMD054I/GBU238LHN9en0X3Bb/5qHkWtX6Yv4p+PAL47agYt6RLKq8Y1S+/BrYdp50b6Xa+Rtr8yBjv7z6j1vuRUXE+YmD60OsfvZ2t/z4w6j26bGB4Z+CmUbG9amD4OcBV7fX/0O28jUzbGLgDWNSGC3jmqHgKeOQq+nxX4MaB4TOBtwwMvxo4deB9/swYbYRuB3a7gXF7AldP5d+pP/74M7d+Rr676YrUf6Ir/r4CzG/fdYvafGcCfzaw3BLgmwPD9/leHD19rHlGTTsK+Pf2+j7f+ePFPLCeMfcP2vAJwDsGpl8FPGdg+A+Ale31XnT5csOB6UcAXxkYfh7dvsy8NrxJW99m48T6WeAvB9r/4ajpR7B6+yt/OjD9X4BjxlnvErp9o5sGfp4EvB94+6h5rwCeMU47y7nvvtjo93SYz8UzB4ZXd/33fGZW971osR05MP9j2vs7b8i+fvl4n7txYt2srX/BwGfvuIHpzwEub68PAi4Yp53LgN8fGF5It18y7r7eXPvxDLo0ufavqk3oktSODFyKnuSlA5c83QTsxH0vVf/JyIuq+nV7uTHt6GJ1R6dHXDPwesvB4aq6G7iW7mjriJ8OvP7NGMOrepjddVW12cDPJ9s6r23rGoxpcJ3XDrzelu6yt5sGtv/NwEPb9FfQnUm9PMl5SZ67injG8slRMV7X1vkfA+u7ga4o3AogyZvSXaZ3c5u+gFG3DqyB0ds87vqHsLrv2eC6r6F7j+B3Px+/An7B+O/V70iyUZL/TnJNklvoDqhslvs+W+AnA69/PRDf1nQ7jKNtQbejuWygj05t4yVpsn2Y7kqfJYy6vH0qJHlikq+luzXoZrqzs2uaY8bbPxjLfb7zuW8+APh5Vd02apnR+eX6qrprYPie9SX5oyTntsuWb6IryobdrmH2V8bLJWM5d1TuP5cu975p1P7G1m3dw+yLrYnRuX/c9Q9h6PdijHVfA6xHtz3D9PVEuX9ekiPbpei30BXwMM6+K8Plfuj66DMD/XMZcBf37hPOeRbo0hSoqq/THVn8V4B09x5/AHgt8ODqLp26mK5gm8iPgQemu79rxDYDr0eKUdq6QvfF+KM134IJXQdsnWTwO2SbUeusgdfX0p0ZHUykm1TVcwCq6rvVXV71ELrLnj/dtnewjdV1LfDno9Z5/6r6Vrr7zf+G7mqAB7b342bufT/GWu+tdAXliIeNMc/obR5z/WuxTauy9cDrbejeI/jdz8cD6C67G++9GsubgEcBT6zuNoSRyz2H+fxeS3fZ/WjX0+1sPHagfxZU95BFSZpUVXUN3cPingOcPMYsw3zHjzt/ktHzfxT4HLB1VS2gu795mO/MtXWf73zumw9gLfJqu0z7JLp9m4e23PlFVp07x41tivZXrgXeOSr3blRVHxtiX2yycv+Y61/bDRvH6Nx/B11+HaavR2/v6OGXAPvRXYGygO7KD1i73D8y7Y9G9dGGVTWV+62zigW6NHWOAp6VZFdgpNj8OXQPWaE7ajuhtlOxFHhbkvWTPJXusqcRnwT2TfL77d7wN9HdszxVhSDAt+mS1l8nWS/JXi2m0fc2j/gOcEu6B5Lcvx2V3SnJHgBJ/jTJFu0I701tmbvo+utu4BFrEOMxwN+mPdAkyYIkf9KmbUJ3adzPgflJ/p7ufroRPwUWjToAsRx4cdvexcCBa7H+qfCaJL/XHrTyZmDkHsmPAi9LsmvbuXoX8O2qWrmKtn7Kfft8E7pi+qbW/j+sRlwnAvskeWGS+UkenGTX9l5/gO7+xYcAJNlq4LkEkjTZXkF3OfKtY0xbDvxxu2LokW3eQaO/Fy8EHtu+Wzekuzx50CbADVV1W5In0BU70+FjwFuSbJHuYZ1/z2rcbz+B9YEN6HLnnekeVvfsgek/BR6cZME4y0/H/soHgFe1KxiS5AHpHti3CRPvi/0U+L3c9z7t5az6c7E6658Kf5rkMUk2ortH/dPtjPua9PVYuf92uqvuNqLbfxjWF4CHJXlDugcXbpLkiW3aMcA72wET2md1v9Voe51ngS5Nkar6Od1ldG+tqkuB99A96OundPcQn70azb2E7j7uG+iKo3suz6uqK4A/pXsI1/V0hfLzquq3k7AZY2ptP5/ugTDXA0cDL62qy8eZ/64W1650ZzCuB46jOyIL3T2BlyT5Fd0D415cVbe1S/neCZzdLoV60mrE+Bm6s/Efb5dmXdzihe6hK18CrqS7BOw27nup16fa718kOb+9fivd0eAb6e6X/+harH8qfJTuoX3fbz/vaHGcThf7SXRXY2wHvHiCto4A/rf1+QvpDjbdn+59O5fuUvShVNUP6M5YvYnu87uc7sFF0F3F8D3g3NZHX6U7Uy9Jk66qrqqqpeNM/ne6+3d/Cvwvv/swrCMY+F6sqivpCqKv0j29e/T/z3418I9JfklXJH+S6fEOuoP6FwEr6B7s9Y7JaLiqfkn3gK9P0uXCl9BdJTAy/XK6AwTfb/205ajlp3x/pb2/r6R7kOmNdDlmSZs20b7YGcAlwE+SXN/GTfS5GHr9U+TDdFds/gTYkPbQ3zXs63+iO7hzU5LD6PY1r6E7634pXf4fSvusPKut9yd0fyN7t8n/Qfe5Oa39fZxLt4+rJlVrcwWpJGmmJVlJ9xCbr850LJIkaeolOZPuAXzHzXQsmlyeQZckSZIkqQcs0CVJkiRJ6gEvcZckSZIkqQc8gy5JkiRJUg/Mn+kANDtsvvnmtWjRopkOQ5IELFu27Pqq2mKm49DsYi6XpP4YL5dboGsoixYtYunS8f4ziSRpOiW5ZqZj0OxjLpek/hgvl3uJuyRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9cD8mQ5As8OKH93MosNPmekwtAorj9x3pkOQJPXY6Fxu3pCk/vEMuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDU16gJ3lYko8nuSrJpUm+mGSHJBdP9bonS5Izk1yR5KIklyf5rySbrUV7v5fk/5J8t/XLfyRZf2D6x9q6/iHJ8oHxByX5dZL12vDOSS5ag/XvleQLaxq/JGluMZeP2Z65XJI06aa0QE8S4DPAmVW1XVU9Bngz8NDJaj/JdF0FcHBV7QLsAtwO/N+aNNL65GTgs1W1PbADsDHwzjb9YcCT27reDmybZJO2+JOBy4HHDwyfvWabI0nSxMzlv8tcLkmaKlOdEPcG7qiqY0ZGVNVy4NqR4STzkrw7yXntSPOft/EbJzk9yflJViTZr41flOSyJEcD5wNbJ3l/kqVJLknytoG2VyZ520AbOw60fXwbd1GSF7Txz05yTpv/U0k2Hr1BVfVb4K+BbZI8ri332STL2voPbeNekeTfB2J5ZZJ/A54J3FZVx7f27gL+Cnh5ko2A04CHtKPtTwHOA57YmtkdeB9dMqf9/laSByT5YOvDCwb6asy+HZRkj7bMIyZ6MyVJc5K5/N5YzOWSpCk11QX6TsCyCeZ5BXBzVe0B7AG8MsnDgduAA6pqN7qdg/e0I9YAjwI+VFWPr6prgL+rqsV0R8SfkWSXgfavb228HzisjXtrW+fO7ej2GUk2B94C7NPmXwq8cayAWyK+ENixjXp5Ve0OLAZen+TBwMeB56ddwga8DDgeeOzoPqmqW4AfAI8Eng9cVVW7VtU3gG8BT07yAOBu4Ezum9TPBv4OOKP14d7Au9v84/UtAEmeDBwD7FdV3x9rWyVJc5653FwuSZom82c6AODZwC5JDmzDC4DtgR8C70rydLpkthX3Xk53TVWdO9DGC9vR7vnAQuAxwMj9XCe338uAP26v9wFePLJwVd2Y5LltubPbvsP6wDmriDsDr1+f5ID2emtg+6o6N8kZwHOTXAasV1UrkjwTqHHaG2v82cCbgG8A51XVVUkemWQLYOOq+n6SZ9PtQIzstGwIbMP4fftb4NHAscCzq+q6MTew69NDAeZtusUqukKSNMeZy+9tz1wuSVpjU12gXwIcOME8AV5XVV++z8hkCbAFsHtV3ZFkJV2yArh1YL6H0x1N36Ml5xMG5oPuHjOAu7h3e8dKoAG+UlUHTbRRSeYBOwOXJdmLbidhz6r6dZIzB9Z/HN19epfTHXGHrk9eMKq9Tel2Bq4CHjJqdefSHTF/KvfuZPyQbqfkWwOxv6CqrhjV7nh9uxfw4xbn44Exk3pVHUuX+Nlg4fZj7XBIktZ95nJzuSRpmkz1Je5nABskeeXIiCR7ANsOzPNl4C9y79NMd2iXdC0AftYS+t6jlhm0KV2SvznJQ4E/GiKu04DXDsT0QLrk+ZQkj2zjNkqyw+gFW5z/BFxbVRe1OG9sCX1H4Ekj81bVt+mS9UuAj7XRpwMbJXlpa28e8B7ghKr69ej1VdUv6e7zW8K9Sf0c4A3cm9S/DLxu5LLBJI8fGD9W3wLcBOxLd2Zjr1X2liRpLjOXm8slSdNkSgv0qirgAOBZ6f4FySXAEdz3KO9xwKXA+en+Xct/0x0dPxFYnGQpcDDdkeux1nEhcAHd0ewPMtyTUN8BPDDJxUkuBPauqp/TJc6Ppft3J+dy731pACe28RcDDwD2a+NPBea3aW9vyw36JHB2Vd04qk/+JMl3gSvp7tF78yriPRvYoKpGHshzDvAI7k3qbwfWAy5qffj2Nn68vqXF8lPgecD7kow8vEaSpHuYywFzuSRpmqTLMZoq6f5H6b9X1ekzHcva2GDh9rXwkKNmOgytwsoj953pECRNkyTL2gPVNA3W1Vxu3pCkmTNeLp+u/zs65yTZLMmVwG9me0KXJGkuMpdLkqZbH57ivk6qqpuA37nvTZIkzQ7mcknSdPMMuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gA+J01B23moBS/13LJIkzVrmcknqP8+gS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDPsVdQ1nxo5tZdPgpMx2GZoGVPiFYkiRJWiOeQZckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6YFoK9CQHJKkkO04w3xuSbDQw/MUkm63B+jZL8uqB4S2TfHp12xlYfmWSFe3n0iTvSLLBWrT32CRnJLkyyXeTvDVJ2rQNknw1yfIkhyT57MByf5vkewPDz0vyuTVY/5Ik/7Wm8UuS5h5z+e+0Zy6XJE266TqDfhDwTeDFE8z3BuCepF5Vz6mqm9ZgfZsB9yT1qrquqg5cg3YG7V1VOwNPAB4BHLsmjSS5P/A54Miq2gF4HPDkgXgfD6xXVbsCXwT2HFh8T+CWJA9pw08Gzl6TOCRJWk3m8sZcLkmaKlNeoCfZGHgK8ApaUk8yL8m/tqPYFyV5XZLXA1sCX0vytTbfyiSbJ/nnUUfRj0jypiQbJzk9yfmtrf3aLEcC27Uj1+9OsijJxW3ZDZMc3+a/IMnebfySJCcnObUdCf+Xsbanqn4FvArYP8mDxoshyduT/OVAzO9s2/gS4OyqOq2192vgtcDhLVl/BNg1yXJgU+DmJI9szWwFnESXzGm/v5VkiyQnJTmv/TylrfMBST7Yxl0w0D+D78++Sc5JsvlQb6gkac4xl98Ts7lckjSl5k/DOvYHTq2qK5PckGQ34InAw4HHV9WdSR5UVTckeSPd0e3rR7XxceAo4Og2/ELgD4HbgAOq6paWlM5Nd5nY4cBO7cg1SRYNtPUagKraOd1leqcl2aFN25XuqPftwBVJ3ltV147eoLa+q4HtgWXjxPA/wMnAfyS5H90OzROAv2vLDLZ3Vdv5uQ34M+Cwqnpui/1bwJOTzAO+C5wL/EGSLwC7AOcBHwT+vaq+mWQb4MvAo9u6zqiql6e7vPA7Sb46st4kBwBvBJ5TVTeO3s4khwKHAszbdIvRkyVJc8f+mMtnfS7fZpttRk+WJPXMdBToB9ElZOiS80F0l5UdU1V3AlTVDatqoKouSPKQJFsCWwA3VtUPkqwHvCvJ04G76Y5KP3SCeJ4KvLe1e3mSa4CRpH56Vd0MkORSYFvgd5J6k4HfvxNDVa1M8oskj28xXVBVv0gSoMbb1DHGnU13dH0ecA7wHeDv6XY+rqiq25LsAzymaxqATZNsAjwbeH6Sw9r4DYGR7Lw3sBh4dlXdMmYwVcfSLv/bYOH248UsSVr3mcvXgVy+ePFic7kk9dyUFuhJHgw8E9gpSdElpqI76ry6SeLTwIHAw+h2DgAOpkvyu1fVHUlW0iWuVYa1imm3D7y+i3H6pyXMRcCVE8RwHLCkxfzBNu4S4Omj2nsE8Kuq+uVAYh7xLeB1dH33gTbPhsBe3HvP2v2APavqN6PaDfCCqrpi1PgnAt+n27naAVg61nZKkmQuN5dLkqbPVN+DfiDwoaratqoWVdXWwNXA+cCrkswHSPKgNv8vgU3GaevjdJeWHUiX4AEWAD9ryXRvuqPkE7VzFl0ipl0Otw1wxTjz/o52+drRwGfbpWTjxQDwGbrL9/agu1QN4ETgqe1I+ciDZv4TGPM+OeBSuvv5ngZc0MYtp7t37ltt+DS6e99GYty1vfwy8LqW3GlnAEZcA/wx8KEkjx1y8yVJc4+53FwuSZomU12gH0SX2AadRJekfgBclORCuoetQHcJ1pfSHiwzqKouoUvUP6qqH7fRJwKLkyylS9SXt3l/AZyd5OIk7x7V1NHAvCQrgE8AS6rqdib2tXQPp/lOi/3PVxVDi+O3wNeAT1bVXW3cb4D9gLckuQJYQXfv2Zj/KqWqCvg2cH1V3dFGn0N3xHwkqb++xXBRu5zvVW3824H16Pr54jY82PYVLeZPJdluiD6QJM095nJzuSRpmqTLGZoK7YEy5wN/UlXfnel41sYGC7evhYccNdNhaBZYeeS+Mx2CtM5LsqyqFs90HHPBupTLFy9eXEuXeiW8JPXBeLl8uv4P+pyT5DHA9+geVjOrE7okSXORuVySNN2m4ynuc1JVXUp36ZokSZqFzOWSpOnmGXRJkiRJknrAAl2SJEmSpB6wQJckSZIkqQe8B11D2XmrBSz16dySJEmSNGU8gy5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD/gUdw1lxY9uZtHhp8x0GJI06630P2Johgzmcj+HktRPnkGXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSemBOFehJHpbk40muSnJpki8m2SHJxTMd27CSnJlk8cDwotkUvyRJa8o8Lkla182f6QCmS5IAnwH+t6pe3MbtCjx0EttPVd09Ge3NhCTzququmY5DkqTRzOMTM49L0uw3l86g7w3cUVXHjIyoquXAtSPDSeYleXeS85JclOTP2/iNk5ye5PwkK5Ls18YvSnJZkqOB84Gtk7w/ydIklyR520DbK5O8baCNHQfaPr6NuyjJC9r4Zyc5p83/qSQbT7SBSTYcaOuCJHu38UuS/NfAfF9Isld7/ask/5jk28Cea9q5kiRNMfP4vfOZxyVpHTVnzqADOwHLJpjnFcDNVbVHkg2As5OcRpf8D6iqW5JsDpyb5HNtmUcBL6uqVwMk+buquiHJPOD0JLtU1UVt3uurarckrwYOA/4MeGtb585t+Qe2dbwF2Keqbk3yN8AbgX9s7ZyY5Dft9frAyNH+1wBU1c5tx+G0JDtMsM0PAC6uqr8fPSHJocChAPM23WKCZiRJmlLm8bGNm8dbPOZySZpF5lKBPoxnA7skObANLwC2B34IvCvJ0+mS6Fbce0ndNVV17kAbL2zJcD6wEHgMMJLYT26/lwF/3F7vA7x4ZOGqujHJc9tyZ3dX3LE+cM7AOg6uqqXQHf0HvtDGPxV4b2vn8iTXABMl9ruAk8aaUFXHAscCbLBw+5qgHUmSZpp5fBRzuSTNLnOpQL8EOHCCeQK8rqq+fJ+RyRJgC2D3qrojyUpgwzb51oH5Hk53RH2PlqBPGJgP4Pb2+y7u7fsAoxNmgK9U1UETb9bvLDeWO7nv7QyDMd3m/WqSpFnAPH4v87gkraPm0j3oZwAbJHnlyIgkewDbDszzZeAvkqzXpu+Q5AF0R+B/1pL63qOWGbQpXaK/OclDgT8aIq7TgNcOxPRA4FzgKUke2cZtNMQlbgBnAQePxA5sA1wBrAR2TXK/JFsDTxiiLUmS+sQ8bh6XpHXenCnQq6qAA4Bnpfv3LJcARwDXDcx2HHApcH66f3ny33RHyE8EFidZSpc4Lx9nHRcCF9Ad5f8gcPYQob0DeGCSi5NcCOxdVT8HlgAfS3IRXaLfcYi2jgbmJVkBfAJYUlW3tziuBlYA/0r3IBxJkmYN87h5XJLmgnT5Tlq1DRZuXwsPOWqmw5CkWW/lkfuudRtJllXV4onnlO41mMsn43MoSVpz4+XyOXMGXZIkSZKkPrNAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkH5s90AJoddt5qAUv9lyySJM1a5nJJ6j/PoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD/iQOA1lxY9uZtHhp8x0GJI0a6304VyaYaNzuZ9JSeofz6BLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPTCnCvQkBySpJDtOMN8bkmw0MPzFJJutwfo2S/LqgeEtk3x6ddsZWH5lks0HhvdK8oU1bU+SpNnGXC5JWpfNqQIdOAj4JvDiCeZ7A3BPUq+q51TVTWuwvs2Ae5J6VV1XVQeuQTtTKp259lmQJM1O5vIxmMslad0wZ77Ik2wMPAV4BS2pJ5mX5F+TrEhyUZLXJXk9sCXwtSRfa/OtTLJ5kn8edRT9iCRvSrJxktOTnN/a2q/NciSwXZLlSd6dZFGSi9uyGyY5vs1/QZK92/glSU5OcmqS7yb5lyG370FJPtu249wkuwzEeNjAfBe3OBYluSzJ0cD5wNZr1cGSJE0xc/k985nLJWkdNX+mA5hG+wOnVtWVSW5IshvwRODhwOOr6s4kD6qqG5K8Edi7qq4f1cbHgaOAo9vwC4E/BG4DDqiqW9pla+cm+RxwOLBTVe0KkGTRQFuvAaiqndtleqcl2aFN2xV4PHA7cEWS91bVtW3a15Lc1V5vDFzeXr8NuKCq9k/yTOBDrZ1VeRTwsqp69QTzSZLUB/tjLh/NXC5J65C5VKAfRJeQoUvOBwGPAI6pqjsBquqGVTVQVRckeUiSLYEtgBur6gdJ1gPeleTpwN3AVsBDJ4jnqcB7W7uXJ7kGGEnqp1fVzQBJLgW2BUaS+j07G0n2Ag4baO8Frb0zkjw4yYIJYrimqs4db2KSQ4FDAeZtusUETUmSNOXM5b/LXC5J65A5UaAneTDwTGCnJAXMAwpY1n6vjk8DBwIPo9s5ADiYLsnvXlV3JFkJbDhRWKuYdvvA67sY7n0aq70C7uS+tzIMxnXrqhqsqmOBYwE2WLj96vaTJEmTxlxuLpekuWCu3IN+IPChqtq2qhZV1dbA1XT3a70qyXzo7v1q8/8S2GSctj5Od9/bgXQJHmAB8LOW0PemO0o+UTtn0e0M0C6H2wa4Yg23b3R7ewHXV9UtwEpgtzZ+N7rLACVJmm3M5ZjLJWldN1cK9IOAz4wadxLdA2R+AFyU5ELgJW3ascCXRh4sM6iqLqFL1D+qqh+30ScCi5MspUusl7d5fwGc3R7m8u5RTR0NzEuyAvgEsKSqbmfNHdFiuIjugTaHDGzng5IsB/4CuHIt1iFJ0kwxl5vLJWmdlyqvdtLENli4fS085KiZDkOSZq2VR+47aW0lWVZViyetQc0Jo3P5ZH4mJUmrZ7xcPlfOoEuSJEmS1GsW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1wPyZDkCzw85bLWCp/45FkqRZy1wuSf3nGXRJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesCnuGsoK350M4sOP2Wmw1hnrPQpupKkaba6udxcJUnTzzPokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoM9RSc5Msnim45AkSavPPC5J6yYLdEmSJEmSesACfZZI8tdJXt9e/3uSM9rr30/ykSTPTnJOkvOTfCrJxm367km+nmRZki8nWTiq3fsl+d8k75j+rZIkaW4wj0uShmGBPnucBTytvV4MbJxkPeCpwArgLcA+VbUbsBR4Y5v+XuDAqtod+CDwzoE25wMnAldW1VtGrzDJoUmWJll6169vnqrtkiRpLpj2PA7mckmabebPdAAa2jJg9ySbALcD59Ml+KcBnwMeA5ydBGB94BzgUcBOwFfa+HnAjwfa/G/gk1U1mOzvUVXHAscCbLBw+5r8TZIkac6Y9jwO5nJJmm0s0GeJqrojyUrgZcC3gIuAvYHtgKuBr1TVQYPLJNkZuKSq9hyn2W8Beyd5T1XdNmXBS5I0x5nHJUnD8BL32eUs4LD2+xvAq4DlwLnAU5I8EiDJRkl2AK4AtkiyZxu/XpLHDrT3P8AXgU8l8WCNJElTyzwuSVolC/TZ5RvAQuCcqvopcBvwjar6ObAE+FiSi+gS/Y5V9VvgQOCfk1xItxPw5MEGq+rf6C6z+3ASPw+SJE0d87gkaZU82jqLVNXpwHoDwzsMvD4D2GOMZZYDTx9j/F4Dr/9hkkOVJEmjmMclSRPxSKskSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPeBT3DWUnbdawNIj953pMCRJ0hoyl0tS/3kGXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQe8CnuGsqKH93MosNPmZK2V/pEWUmSptx4udw8LEn94Rl0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0OeAJJslefXA8F5JvjCTMUmSpOGZyyVpbrBAnxs2A1490UySJKm3NsNcLknrPAv0nkmyKMnlSY5LcnGSE5Psk+TsJN9N8oQkD0ry2SQXJTk3yS5t2SOSfDDJmUm+n+T1rdkjge2SLE/y7jZu4ySfbus6MUlmZIMlSVrHmMslSWtq/kwHoDE9EvgT4FDgPOAlwFOB5wNvBq4FLqiq/ZM8E/gQsGtbdkdgb2AT4Iok7wcOB3aqql2huywOeDzwWOA64GzgKcA3B4NIcmiLgXmbbjEV2ylJ0rrKXC5JWm2eQe+nq6tqRVXdDVwCnF5VBawAFtEl+A8DVNUZwIOTLGjLnlJVt1fV9cDPgIeOs47vVNUP2zqWt3bvo6qOrarFVbV43kYLRk+WJEnjM5dLklabBXo/3T7w+u6B4bvprnoY6xK2GmPZuxj/Kolh55MkSavPXC5JWm0W6LPTWcDBcM8lbtdX1S2rmP+XdJfJSZKkfjCXS5J+h0daZ6cjgOOTXAT8GjhkVTNX1S/ag2kuBr4EnDL1IUqSpFU4AnO5JGmUdLdDSau2wcLta+EhR01J2yuP3HdK2pWkdVWSZVW1eKbj0OwyXi43D0vS9Bsvl3uJuyRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIP+H/QNZSdt1rAUv8NiyRJs5a5XJL6zzPokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDPiROQ1nxo5tZdPgpa7TsSh9II0nSjFubXD6XuN8iaSZ5Bl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQv0aZbk75JckuSiJMuTPHEK1/WqJC9tr5ck2XKq1iVJ0lxhLpckTZX5Mx3AXJJkT+C5wG5VdXuSzYH1p2hd86vqmIFRS4CLgeumYn2SJM0F5nJJ0lTyDPr0WghcX1W3A1TV9VV1XZLdk3w9ybIkX06yMMmjk3xnZMEki5Jc1F7/zvxt/JlJ3pXk68BfJjkiyWFJDgQWAye2I/37JvnMQNvPSnLydHaEJEmzlLlckjRlLNCn12nA1kmuTHJ0kmckWQ94L3BgVe0OfBB4Z1VdBqyf5BFt2RcBnxxv/oF1bFZVz6iq94yMqKpPA0uBg6tqV+CLwKOTbNFmeRlw/FRttCRJ6xBzuSRpyniJ+zSqql8l2R14GrA38AngHcBOwFeSAMwDftwW+STwQuBIuqT+IuBRq5if1uZEcVSSDwN/muR4YE/gpaPnS3IocCjAvE23GD1ZkqQ5x1wuSZpKFujTrKruAs4EzkyyAngNcElV7TnG7J8APtUuWauq+m6SnVcxP8CtQ4ZyPPB54DbgU1V15xixHgscC7DBwu1ryHYlSVqnmcslSVPFS9ynUZJHJdl+YNSuwGXAFu2hMyRZL8ljAarqKuAu4K3cezT9ivHmn8AvgU1GBqrqOrqHzLwFOGEtNkuSpDnDXC5JmkqeQZ9eGwPvTbIZcCfwPbrLzo4F/jPJArr35CjgkrbMJ4B3Aw8HqKrftgfFjDf/eE4AjknyG2DPqvoNcCKwRVVdOknbJ0nSus5cLkmaMqnyaqe5Ksl/ARdU1f9MNO8GC7evhYcctUbrWXnkvmu0nCRpbEmWVdXimY5DM2+6cvlc4n6LpOkwXi73DPoclWQZ3T1ub5rpWCRJ0uozl0vSuscCfY5q/9ZFkiTNUuZySVr3+JA4SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBHxKnoey81QKW+m9HJEmatczlktR/nkGXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQd8iruGsuJHN7Po8FNmOgxJq7DSpzNLWgVzuSRNjqnc5/IMuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuhrIMkBSSrJjm14UZKLp2hdRyQ5bDXm/9VUti9J0rrAXC5J6iML9DVzEPBN4MVrsnCSeZMbjiRJWk3mcklS71igr6YkGwNPAV7BGEm9HYH/RpLz28+T2/i9knwtyUeBFW3460k+meTKJEcmOTjJd5KsSLLdBHF8NsmyJJckOXTUtPe0dZ+eZIs2brskp7ZlvjFyxkCSpLnGXC5J6isL9NW3P3BqVV0J3JBkt1HTfwY8q6p2A14E/OfAtCcAf1dVj2nDjwP+EtgZ+P+AHarqCcBxwOsmiOPlVbU7sBh4fZIHt/EPAM5v6/868A9t/LHA69oyhwFHT7ShSQ5NsjTJ0rt+ffNEs0uSNFvsj7lcktRD82c6gFnoIOCo9vrjbfh9A9PXA/4rya7AXcAOA9O+U1VXDwyfV1U/BkhyFXBaG78C2HuCOF6f5ID2emtge+AXwN3AJ9r4jwAntzMFTwY+lWRk+Q0maJ+qOpZuZ4ANFm5fE80vSdIsYS6XJPWSBfpqaEe2nwnslKSAeUBx3yPYfwX8lO6I+v2A2wam3TqqydsHXt89MHw3q3hvkuwF7APsWVW/TnImsOE4s1eL46aq2nW8NiVJmgvM5ZKkPvMS99VzIPChqtq2qhZV1dbA1cDvDcyzAPhxVd1Nd6nbVDxEZgFwY0voOwJPGph2vxYnwEuAb1bVLcDVSf4EIJ3HTUFckiT1nblcktRbFuir5yDgM6PGnQS8eWD4aOCQJOfSXRI3+kj7mnhLkh+O/ACnAvOTXAS8HTh3YN5bgccmWUZ3huAf2/iDgVckuRC4BNhvEuKSJGm2MZdLknorVd6OpIltsHD7WnjIUTMdhqRVWHnkvjMdgqZJkmVVtXim49DsYi6XpMkxGftc4+Vyz6BLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1APzZzoAzQ47b7WApT4hWpKkWctcLkn95xl0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAp7hrKCt+dDOLDj9lpsPohZU+AVeSNAuZyzVd3FeS1pxn0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQesECfREkOSFJJdpzENvdP8pj2+n1Jlie5NMlv2uvlSQ6crPVJkjSXmcslSTPJAn1yHQR8E3jxJLa5P/AYgKp6TVXtCjwHuKqqdm0/n57E9UmSNJeZyyVJM8YCfZIk2Rh4CvAKWlJPsjDJWe3I+MVJnpZkXpIT2vCKJH/V5t0uyalJliX5RpIdkzwZeD7w7tbGdmOs98NJ9hsYPjHJ85MsSfJ/rc0rkvzDwDx/muQ7rc3/TjJvirtHkqTeM5dLkmba/JkOYB2yP3BqVV2Z5IYkuwF7A1+uqne2xLkRsCuwVVXtBJBks7b8scCrquq7SZ4IHF1Vz0zyOeALqziyfhzwV8D/JVkAPBk4BPhT4AnATsCvgfOSnALcCrwIeEpV3ZHkaOBg4EOjG05yKHAowLxNt1iLrpEkaVbYH3O5JGkGWaBPnoOAo9rrj7fhzwMfTLIe8NmqWp7k+8AjkrwXOAU4rR2xfzLwqSQj7W0wzEqr6uvtfraHAH8MnFRVd7Z2vlJVvwBIcjLwVOBOYHe6JA9wf+Bn47R9LN3OBhss3L6G7QhJkmYpc7kkaUZZoE+CJA8GngnslKSAeUABfw08HdgX+HCSd1fVh5I8DvgD4DXAC4E3ADe1e9LWxIfpjpy/GHj5wPjRibiAAP9bVX+7huuSJGmdYy6XJPWB96BPjgOBD1XVtlW1qKq2Bq6mS+g/q6oPAP8D7JZkc+B+VXUS8FZgt6q6Bbg6yZ8ApPO41vYvgU0mWP8JdDsGVNUlA+OfleRBSe5Pd9ne2cDpwIHtKD1t+rZrt/mSJM165nJJ0ozzDPrkOAg4ctS4k+iS7a1J7gB+BbwU2Ao4PsnIwZGRo98HA+9P8hZgPbpL6y5svz+Q5PXAgVV11eiVV9VPk1wGfHbUpG/SHZF/JPDRqloK0NZxWovhDrqj/9es2aZLkrROMJdLkmZcqrwdabZLshGwgu4I/s1t3BJgcVW9djLWscHC7WvhIUdNRlOz3soj953pECTNcUmWVdXimY5Dk8dcrnWJ+0rSxMbL5V7iPssl2Qe4HHjvSEKXJEmzh7lckjTCS9xnuar6KrDNGONPoLssT5Ik9Zi5XJI0wjPokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDPiROQ9l5qwUs9V9mSJI0a5nLJan/PIMuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST3gQ+I0lBU/uplFh58y02GoB1b6gCFJmpXM5XOb+VuaHTyDLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPVA7wr0JFsm+fQaLLdZklevbTvTJcmSJFtO8zrPTLJ4OtcpSZpbzONTuk7zuCSt49a6QE9n0gr9qrquqg5cg0U3A+5J7GvRznRZAoyZ2JPMm95QJElzlXl8jS3BPC5JmmRDJeQkb0xycft5Q5JFSS5LcjRwPrB1krcmuTzJV5J8LMlhbdlXJjkvyYVJTkqyURt/QpL/TPKtJN9PcmAbvyjJxe31cUmWt5+fJ/mHJBsnOT3J+UlWJNmvhXkksF2b992j2tkwyfFt/guS7N3GL0lycpJTk3w3yb8MbPMftnVcmOT0Nu5BST6b5KIk5ybZpY0/YmR72/DFbf0j/fSBJJckOS3J/du2LgZObPHeP8nKJH+f5JvA4UnOH2hv+yTL2us9Wp9dmOQ7STZZxfbdP8nHW7yfAO4/0Oazk5zTtvFTSTYe9kMjSZpdzOPmcUnS7DBhgZ5kd+BlwBOBJwGvBB4IPAr4UFU9HtgCeAHweOCP6ZLWiJOrao+qehxwGfCKgWkLgacCz6VLzPdRVX9WVbsC+wG/AE4AbgMOqKrdgL2B9yQJcDhwVVXtWlX/b1RTr2nt7QwcBPxvkg3btF2BFwE7Ay9KsnWSLYAPAC9ocf9Jm/dtwAVVtQvwZuBDE/UfsD3wvqp6LHBTa/PTwFLg4Bbvb9q8t1XVU6vqncDNSXZt418GnJBkfeATwF+2uPYBfrOK7fsL4Nct3ncCuwMk2Rx4C7BP68elwBuH2BZJ0ixjHjePS5Jmj/lDzPNU4DNVdStAkpOBpwHXVNW5A/P830iCSvL5geV3SvIOukvXNga+PDDts1V1N3BpkoeOtfKWoD4FvLaqrkmyHvCuJE8H7ga2AsZcdtQ2vBegqi5Pcg2wQ5t2elXd3NZ1KbAt3Y7LWVV1dVvmhoF2XtDGnZHkwUkWTLDuq6tqeXu9DFi0ink/MfD6OOBlSd5It+PxBLqdqR9X1Xkthlta3ONt39OB/2zjL0pyUWv7ScBjgLO7fSLWB84ZHUySQ4FDAeZtusUEmylJ6inz+BzN461tc7kkzSLDFOgZZ/ytQ8wD3dHy/avqwiRLgL0Gpt0+RBvH0B29/2obPpjuSP/uVXVHkpXAhuMsO0x8gzHcRdcnAWrIdgq4k/tejTAYz+j278/4Bvv0JOAfgDOAZVX1iyRbrUZcg/GNNf9XquqgVSxHVR0LHAuwwcLtx2pHktR/5vFVt7PO5nEwl0vSbDPMPehnAfsn2SjJA4ADgG+MmuebwPPaPVQbA/sOTNsE+HE7Yn7w6gSX5DXAJlU1eNncAuBnLanvTXekHOCXbV3jbcPBrc0dgG2AK1ax6nOAZyR5eFvmQWO0sxdwfTv6vRLYrY3fDXj4EJu3qnipqtvozlK8Hzi+jb4c2DLJHm1dmySZv4rtGxy/E7BLa+dc4ClJHtmmbdSWkySte8zj5nFJ0iwx4Rn0qjo/yQnAd9qo44AbR81zXpLPARcC19DdC3Vzm/xW4Ntt/ApWkczGcBhwR5LlbfgY4ETg80mWAsvpkh3tyPTZ6R4o8yXgfQPtHA0ck2QF3VHyJVV1e7ssbKxt/nm7JOzkdE+2/RnwLOAI4Ph2idmvgUPaIicBL21xngdcOcS2ndBi+g2w5zjznEh3L+BpLa7fJnkR8N4k96e7b22fVWzf+wfiXU57D9v2LQE+lmSDtq63DBm3JGkWMY+bxyVJs0eqJudqpyQbV9Wv0j3d9Szg0Ko6f6LlNL50T5RdUFVvnelYNli4fS085KiZDkM9sPLIfSeeSdKUSrKsqib1/2Gbxydfn/I4mMvnOvO31C/j5fJh7kEf1rFJHkN339b/mtTXTpLPANsBz5zpWCRJc4J5fBKZxyVJa2LSCvSqeslktSWoqgNmOgZJ0txhHp9c5nFJ0poY5iFxkiRJkiRpilmgS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9MJlPcdc6bOetFrDUf88hSdKsZS6XpP7zDLokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST2QqprpGDQLJPklcMVMxzELbA5cP9NBzAL208Tso+HM1X7atqq2mOkgNLvMglw+G/6e+x6j8a0d41t7fY+xT/GNmcvnz0QkmpWuqKrFMx1E3yVZaj9NzH6amH00HPtJWi29zuWz4e+57zEa39oxvrXX9xj7Hh94ibskSZIkSb1ggS5JkiRJUg9YoGtYx850ALOE/TQc+2li9tFw7CdpeH3/e+l7fND/GI1v7Rjf2ut7jH2Pz4fESZIkSZLUB55BlyRJkiSpByzQJUmSJEnqAQt0keQPk1yR5HtJDh9jepL8Z5t+UZLdhl12XbGWfbQyyYoky5Msnd7Ip9cQ/bRjknOS3J7ksNVZdl2ylv00Jz5PQ/TRwe1v7aIk30ryuGGXldZ1a5OzehLfuN+BPYlv3O+fnsS3X4tteZKlSZ46nfENE+PAfHskuSvJgX2KL8leSW5ufbg8yd/3Kb6BGJcnuSTJ1/sUX5L/N9B3F7f3+EE9i3FBks8nubD14cumM75Vqip/5vAPMA+4CngEsD5wIfCYUfM8B/gSEOBJwLeHXXZd+FmbPmrTVgKbz/R29KSfHgLsAbwTOGx1ll1Xftamn+bK52nIPnoy8MD2+o/m2veSP/6M97O2Oasn8Y37HdiT+Mb8/ulRfBtz73OmdgEu71sfDsx3BvBF4MA+xQfsBXxhOvttNePbDLgU2KYNP6RP8Y2a/3nAGT3swzcD/9xebwHcAKw/E+/56B/PoOsJwPeq6vtV9Vvg48B+o+bZD/hQdc4FNkuycMhl1wVr00dzyYT9VFU/q6rzgDtWd9l1yNr001wxTB99q6pubIPnAr837LLSOq7vOavv34Fr8/3Tl/h+Va3qAB4ATPcToYf9Hn4dcBLws+kMjv7niWHiewlwclX9ALq/mZ7FN+gg4GPTEtm9homxgE2ShO6g1g3AndMb5tgs0LUVcO3A8A/buGHmGWbZdcHa9BF0XwCnJVmW5NApi3Lmrc3nYa58lmDtt3UufJ5Wt49eQXc2cE2WldY1a5uzplrf/0bX5vtnOgwVX5IDklwOnAK8fJpiGzFhjEm2Ag4AjpnGuEYM+x7v2S5//lKSx05PaMBw8e0APDDJmW1/4KXTFt1q/I0k2Qj4Q7oDMdNpmBj/C3g0cB2wAvjLqrp7esJbtfkzHYBmXMYYN/pI63jzDLPsumBt+gjgKVV1XZKHAF9JcnlVnTWpEfbD2nwe5spnCdZ+W+fC52noPkqyN90O8sg9lnPpsySNZW1z1lTr+9/o2nz/TIeh4quqzwCfSfJ04O3APlMd2IBhYjwK+Juquqs7gTmthonvfGDbqvpVkucAnwW2n+rAmmHimw/sDvw+cH/gnCTnVtWVUx0cq/c3/Dzg7Kq6YQrjGcswMf4BsBx4JrAd3T7VN6rqlimObUKeQdcPga0Hhn+P7kjSMPMMs+y6YG36iKoa+f0z4DN0l92si9bm8zBXPkuwlts6Rz5PQ/VRkl2A44D9quoXq7OstA5bq5w1Dfr+N7o23z/TYbX6rx3A3S7J5lMd2IBhYlwMfDzJSuBA4Ogk+09LdEPEV1W3VNWv2usvAutNYx8O+zd8alXdWlXXA2cB0/WwwtX5DL6Y6b+8HYaL8WV0twlUVX0PuBrYcZriWyULdJ0HbJ/k4UnWp/tD+tyoeT4HvLQ99fVJwM1V9eMhl10XrHEfJXlAkk0AkjwAeDZw8XQGP43W5vMwVz5LsBbbOoc+TxP2UZJtgJOB/2/UGYO59FmSxrI2eb0v8c2ktfn+6Ut8j2z31ZLuCf3rA9N5EGHCGKvq4VW1qKoWAZ8GXl1Vn+1LfEkeNtCHT6CrmaarD4f5G/k/4GlJ5rfLyJ8IXNaj+EiyAHhGi3W6DRPjD+iuQCDJQ4FHAd+f1ijH4SXuc1xV3ZnktcCX6Z54+MGquiTJq9r0Y+iervkc4HvAr+mOOI277AxsxpRamz4CHkp3iRl0f28frapTp3kTpsUw/ZTkYcBSYFPg7iRvoHuq5i1z4bMEa9dPwObMgc/TkH9zfw88mO6sC8CdVbV4rnwvSeNZy5zVi/hWlSv6EB/jfP9MdWyrEd8L6A7A3AH8BnjRwEPj+hLjjBkyvgOBv0hyJ10fvni6+nCY+KrqsiSnAhcBdwPHVdW0HLBfjff3AOC0qrp1OuJagxjfDpyQZAXdJfF/065GmHGZxr9XSZIkSZI0Di9xlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrg/wdp8RteZ86yawAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-23 13:59:07,826] A new study created in memory with name: no-name-aea0ee09-f32c-41a2-9842-0c87dc616b1b\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 75) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_2292/954963820.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "[I 2024-06-23 13:59:20,879] Trial 0 finished with value: 0.3750848306832929 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.04320360301136058, 'learning_rate': 'adaptive', 'max_iter': 320}. Best is trial 0 with value: 0.3750848306832929.\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 75) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_2292/954963820.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "[I 2024-06-23 13:59:25,036] Trial 1 finished with value: 0.004918341481204307 and parameters: {'hidden_layer_sizes': (100,), 'activation': 'tanh', 'solver': 'sgd', 'alpha': 8.588909205308777e-05, 'learning_rate': 'constant', 'max_iter': 315}. Best is trial 1 with value: 0.004918341481204307.\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 75) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_2292/954963820.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "[I 2024-06-23 13:59:29,826] Trial 2 finished with value: 0.29339805050214396 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.013705225242083237, 'learning_rate': 'constant', 'max_iter': 422}. Best is trial 1 with value: 0.004918341481204307.\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 75) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_2292/954963820.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "[I 2024-06-23 13:59:36,825] Trial 3 finished with value: 0.004461938142871143 and parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'tanh', 'solver': 'sgd', 'alpha': 0.000977435524827468, 'learning_rate': 'constant', 'max_iter': 405}. Best is trial 3 with value: 0.004461938142871143.\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100,) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (100, 50) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (150, 75) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/distributions.py:524: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (200, 100) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "/tmp/ipykernel_2292/954963820.py:100: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/neural_network/_base.py:172: RuntimeWarning: overflow encountered in square\n",
      "  return ((y_true - y_pred) ** 2).mean() / 2\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py:208: RuntimeWarning: overflow encountered in matmul\n",
      "  ret = a @ b\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/utils/extmath.py:208: RuntimeWarning: invalid value encountered in matmul\n",
      "  ret = a @ b\n",
      "/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (487) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "[W 2024-06-23 14:01:54,116] Trial 4 failed with parameters: {'hidden_layer_sizes': (100, 50), 'activation': 'relu', 'solver': 'sgd', 'alpha': 4.701424107929805e-05, 'learning_rate': 'constant', 'max_iter': 487} because of the following error: ValueError('Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.').\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patil.anjali/.local/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_2292/954963820.py\", line 114, in objective\n",
      "    model.fit(X_train, y_train)\n",
      "  File \"/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 751, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"/home/patil.anjali/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 495, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "[W 2024-06-23 14:01:54,119] Trial 4 failed with value None.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Run the optimization\u001b[39;00m\n\u001b[1;32m    123\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Get the best hyperparameters\u001b[39;00m\n\u001b[1;32m    127\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m    105\u001b[0m model \u001b[38;5;241m=\u001b[39m MLPRegressor(\n\u001b[1;32m    106\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39mhidden_layer_sizes,\n\u001b[1;32m    107\u001b[0m     activation\u001b[38;5;241m=\u001b[39mactivation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[1;32m    113\u001b[0m )\n\u001b[0;32m--> 114\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Predict and evaluate on validation data\u001b[39;00m\n\u001b[1;32m    117\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:751\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    735\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;124;03m        Returns a trained MLP model.\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mincremental\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:495\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[0;34m(self, X, y, incremental)\u001b[0m\n\u001b[1;32m    493\u001b[0m weights \u001b[38;5;241m=\u001b[39m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoefs_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercepts_)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(np\u001b[38;5;241m.\u001b[39misfinite(w)\u001b[38;5;241m.\u001b[39mall() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights):\n\u001b[0;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    496\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolver produced non-finite parameter weights. The input data may\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m contain large values and need to be preprocessed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy import stats\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Sample the data for quick processing\n",
    "df_sample = df.sample(n=100000, random_state=42)\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'ActivatedTimestamp', 'ClearedTimestamp', 'month', 'week', 'ResolutionTimeMinutes'\n",
    "]\n",
    "df_selected = df_sample[selected_columns]\n",
    "\n",
    "# Convert categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity']:\n",
    "    le = LabelEncoder()\n",
    "    df_selected[column] = le.fit_transform(df_selected[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Convert timestamps to datetime and extract features\n",
    "df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
    "df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
    "df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
    "df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
    "df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
    "df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
    "df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n",
    "\n",
    "# Remove rows with invalid resolution times\n",
    "df_selected = df_selected[(df_selected['ResolutionTime'] >= 0) & (df_selected['ResolutionTime'] <= 10000)]\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = np.abs(stats.zscore(df_selected['ResolutionTimeMinutes']))\n",
    "df_selected = df_selected[z_scores < 3]\n",
    "\n",
    "# Normalize the ResolutionTimeMinutes column\n",
    "scaler = MinMaxScaler()\n",
    "df_selected['ResolutionTimeMinutes'] = scaler.fit_transform(df_selected[['ResolutionTimeMinutes']])\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'month', 'week', 'ActivationHour', 'ClearanceHour',\n",
    "    'ActivationDayOfWeek', 'ClearanceDayOfWeek'\n",
    "]\n",
    "X = df_selected[features]\n",
    "y = df_selected['ResolutionTimeMinutes']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# Mutual Information for feature importance\n",
    "mi = mutual_info_regression(X_train, y_train)\n",
    "mi_importances = mi\n",
    "\n",
    "# Plot feature importances\n",
    "feature_names = X_train.columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "axs[0].barh(feature_names, rf_importances)\n",
    "axs[0].set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Mutual Information Feature Importance\n",
    "axs[1].barh(feature_names, mi_importances)\n",
    "axs[1].set_title('Mutual Information Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove less important columns\n",
    "important_features = ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity', 'month', 'week', 'ActivationHour', 'ClearanceHour']\n",
    "X_train = X_train[important_features]\n",
    "X_val = X_val[important_features]\n",
    "X_test = X_test[important_features]\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    hidden_layer_sizes = tuple(trial.suggest_categorical('hidden_layer_sizes', [(100,), (100, 50), (150, 75), (200, 100)]))\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'adaptive'])\n",
    "    max_iter = trial.suggest_int('max_iter', 300, 500)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Run the optimization\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Train the final model using the best hyperparameters\n",
    "best_nn_model = MLPRegressor(\n",
    "    hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "    activation=best_params['activation'],\n",
    "    solver=best_params['solver'],\n",
    "    alpha=best_params['alpha'],\n",
    "    learning_rate=best_params['learning_rate'],\n",
    "    max_iter=best_params['max_iter'],\n",
    "    random_state=42\n",
    ")\n",
    "best_nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on test data\n",
    "nn_pred_test = best_nn_model.predict(X_test)\n",
    "nn_mse_test = mean_squared_error(y_test, nn_pred_test)\n",
    "nn_r2_test = r2_score(y_test, nn_pred_test)\n",
    "\n",
    "print(f\"Neural Network - Test MSE: {nn_mse_test}, R²: {nn_r2_test}\")\n",
    "\n",
    "# Convert predicted values back to original scale\n",
    "nn_pred_test_original = scaler.inverse_transform(nn_pred_test.reshape(-1, 1))\n",
    "\n",
    "# Display a few predicted values\n",
    "print(f\"Original Scale Predictions - Neural Network: {nn_pred_test_original[:5].ravel()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c94937-dd43-4d2e-9d23-1d7811fcbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to remove above warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01ab556e-9b28-45ce-bfaf-52a00cc887ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2292/2684318520.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/2684318520.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/2684318520.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/2684318520.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected[column] = le.fit_transform(df_selected[column])\n",
      "/tmp/ipykernel_2292/2684318520.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
      "/tmp/ipykernel_2292/2684318520.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
      "/tmp/ipykernel_2292/2684318520.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/2684318520.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
      "/tmp/ipykernel_2292/2684318520.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/2684318520.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
      "/tmp/ipykernel_2292/2684318520.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+gAAAFgCAYAAAAo31N4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABB70lEQVR4nO3deZglZXn///fHGRYRGFRQBwKMIogKiDCouIJBk4gKRKIi+cmokRi3GOWbEOOCcQmJMSExIkEiRMUdNCqKKIgogjIDwwy7IoMobsimKMhy//6op+HQdE93z/RSPf1+XVdffWp76q46p89dd9VT1akqJEmSJEnSzLrfTAcgSZIkSZIs0CVJkiRJ6gULdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0aRZKckSSj810HJIkaWKSLEpSSeZPQlsPTXJWkl8ned9kxDeZkrw5yXEzHYc0m1igS5Mkyaokv0vymyQ/S3JCko1nOq61kWSvJHe1bRr6+eI0rn/Mg5h2suL2YTH+7Vqud1pPgEzmwdpkaLE8cqbjkKS11XLz75NsPmz88vZdt2ic7Uzb92KLeZ9xzn4ocB2waVW9aQrDGlM7Zvjx4Liqek9V/cUUrGtJkjuH5f7/moQ2vz1ZMY5znb3JtxP83GkKWaBLk+t5VbUxsCvweODvZzacSXFtVW088PO8iTaQZN5UBDbgU8Ni/JcpXt9q9aXQnqjZGrckjeEq4KChgSQ7A/efuXAm1bbAJVVVE11wHfjOP2dY7n/tTAYzW/fnbI17XWaBLk2BqvoZ8FW6Qh2AJIcnubJ1Q7skyQED05Yk+XaSf01yQ5KrkvzJwPSHJ/lmW/ZrwPArAc9PcnGSG5OcmeTRA9NWJfl/SVYkuSXJ/7QucV9p7X09yQMnuo1JHt3WdWNb9/MHpp2Q5INJvpzkFmDvJFsmOSnJL9v2vX5g/ickWZrk5iQ/T/JvbdJZ7feN7ez4nhOM8eVJLm379KtJth2Y9h9JrmnrXJbkaW38HwNvBl7U1nnhwH7cZ2D5u6+yD1wBf0WSHwFnjLX+MeI+IcnR7T36TZKzkzwsyVGtrcuSPH5g/lVJ/r59rm5IcnySDQemvzLJD5Jcn+QLSbYcmFZJXpPk+8D3kwzt8wvbul+U5IFJvtTeuxva6z8YaOPMJO9scf46yWkZuFqV5KlJvtM+K9ckWdLGb9A+8z9q7/sxSdaVg2ZJ/fFR4KUDw4cAHxmcoX2P/cXA8N1XU0f5XrzP1dYMXA1Nsm+SC1qOuSbJEWsSeFZzfJDkhLYtf9vi2qd9rx6V5Nr2c1SSDdr8eyX5cZK/S/Iz4PiWyz6T5GPt+3tlkh1aTvlFi/3ZA/G8rOW1Xyf5YZK/bOMfAHwF2DL3XNHeMsN6pGXs45XD0h2v3JTkU4O5bAL77Lnpekjc2HLPLgPTRjwWa3EcA+zZYr+xjR/1c9GG75VDx1r/GHFP9L04M8k/Jfle21//l+RBE9jXf5dkBXBLkk8A2wBfzEBPxBbPz1r7ZyV57EAbJyT5QJJTWrzfTbLdwPTHJvlaumOPnyd5cxt/v4H34VdJPj0YtyzQpSmRrnj5E+AHA6OvBJ4GLADeAXwsycKB6U8ELqcrvv8F+J8kadM+Dixr095Jl5CH1rUD8AngDcAWwJfpvmDXH2j7BcCzgB2A59El0Te39u4HvJ4JSLIe8EXgNOAhwOuAE5M8amC2lwDvBjYBvtPmvxDYCvhD4A1J/qjN+x/Af1TVpsB2wKfb+Ke335u1s+PnTCDG/ds2/indfvkW3X4ach7dCZQH0e3fzyTZsKpOBd7DPVflHzfedQLPAB4N/NE41j+WFwJvoXuPbgPOAc5vw58F/m3Y/AcDf0S3/3Zoy5LkmcA/tfYWAlcDnxy27P50n7/HVNXQPn9c2/5P0X1Gjqe7UrMN8DtgeFfClwAvo/s8rA8c1ta/Dd3n7f1tP+wKLG/L/HOLdVfgkXSfjbeNvWskaULOBTZNd2J5HvAiYNy3MY3yvTiWW+hOCmwG7Av8VcsLa2LE44OqWgKcCPxLi+vrwD8AT6L7Xn0c8ARaPmgeRpf3tqXrHg/dccFHgQcCF9BdYLgf3XfyPwL/PbD8L4DnApvSfef/e5LdquoWuuOewV531w5uxDiPV14I/DHwcGAXYMlEdlSS3YAPA38JPLjF/oW0kxSMcixWVZcCr+Keq/KbTWC1+9Ny6DjWP5aJvBfQfcZeDmwJ3AH8J4x7Xx9E99ncrKoOAn5E6wk60BPxK8D2dLn9fLrPG8PaeEeL9wd0x30k2QT4OnBqi+2RwOltmdfT7bNntGk3AB8Y5/6ZG6rKH3/8mYQfYBXwG+DXQNF9EW22mvmXA/u110uAHwxM26i18TC6gugO4AED0z8OfKy9fivw6YFp9wN+Auw1ENfBA9NPAj44MPw64POjxLgXcBdw48DPC+mS28+A+w3M+wngiPb6BOAjA9OeCPxoWNt/DxzfXp9F9wW/+bB5FrX9MH81+/EI4PfDYtySLqm8Yth++S2w7Sjt3EB38DXU5sdGeH/3Gbbejw2L8xED08e9/uHb2fbfh4a9R5cODO8M3DgstlcNDD8HuLK9/h+6g7ehaRsDtwOL2nABzxwWTwGPXM0+3xW4YWD4TOAtA8OvBk4deJ8/N0IboTuA3W5g3J7AVVP5d+qPP/7MrZ+h7266IvWf6Iq/rwHz23fdojbfmcBfDCy3BPj2wPC9vheHTx9pnmHTjgL+vb2+13f+aDEPrGfE44M2fALwroHpVwLPGRj+I2BVe70XXb7ccGD6EcDXBoafR3csM68Nb9LWt9kosX4e+OuB9n88bPoRTOx45c8Hpv8LcMwo611Cd2x048DPk4APAu8cNu/lwDNGaWc59z4WG/6ejudz8cyB4Ymu/+7PzETfixbbkQPzP6a9v/PGua9fPtrnbpRYN2vrXzDw2TtuYPpzgMva64OAC0Zp51LgDweGF9Idl4x6rDfXfryCLk2u/atqE7oktSMDXdGTvHSgy9ONwE7cu6v6z4ZeVNVv28uNaWcXqzs7PeTqgddbDg5X1V3ANXRnW4f8fOD170YYXt3D7K6tqs0Gfj7d1nlNW9dgTIPrvGbg9bZ03d5uHNj+NwMPbdNfQXcl9bIk5yV57mriGcmnh8V4bVvnfwys73q6onArgCRvStdN76Y2fQHDbh1YA8O3edT1j8NE37PBdV9N9x7BfT8fvwF+xejv1X0k2SjJfye5OsnNdCdUNsu9ny3ws4HXvx2Ib2u6A8bhtqA70Fw2sI9ObeMlabJ9lK6nzxKGdW+fCkmemOQb6W4Nuonu6uya5pjRjg9Gcq/vfO6dDwB+WVW3DltmeH65rqruHBi+e31J/iTJua3b8o10Rdl4t2s8xyuj5ZKRnDss959Ll3vfNOx4Y+u27vEci62J4bl/1PWPw7jfixHWfTWwHt32jGdfj5X75yU5snVFv5mugIdRjl0ZX+6Hbh99bmD/XArcyT3HhHOeBbo0Barqm3RnFv8VIN29xx8CXgs8uLquUxfRFWxj+SnwwHT3dw3ZZuD1UDFKW1fovhh/suZbMKZrga2TDH6HbDNsnTXw+hq6K6ODiXSTqnoOQFV9v7ruVQ+h6/b82ba9g21M1DXAXw5b5/2r6jvp7jf/O7reAA9s78dN3PN+jLTeW+gKyiEPG2Ge4ds84vrXYptWZ+uB19vQvUdw38/HA+i63Y32Xo3kTcCjgCdWdxvCUHfP8Xx+r6Hrdj/cdXQHG48d2D8LqnvIoiRNqqq6mu5hcc8BTh5hlvF8x486f5Lh838c+AKwdVUtoLu/eTzfmWvrXt/53DsfwFrk1dZN+yS6Y5uHttz5ZVafO0eNbYqOV64B3j0s925UVZ8Yx7HYZOX+Ede/ths2iuG5/3a6/DqefT18e4cPvwTYj64HygK6nh+wdrl/aNqfDNtHG1bVVB63zioW6NLUOQp4VpJdgaFi85fQPWSF7qztmNpBxVLgHUnWT/JUum5PQz4N7JvkD9u94W+iu2d5qgpBgO/SJa2/TbJekr1aTMPvbR7yPeDmdA8kuX87K7tTkj0Akvx5ki3aGd4b2zJ30u2vu4BHrEGMxwB/n/ZAkyQLkvxZm7YJXde4XwLzk7yN7n66IT8HFg07AbEceHHb3sXAgWux/qnwmiR/0B608mZg6B7JjwMvS7JrO7h6D/Ddqlq1mrZ+zr33+SZ0xfSNrf23TyCuE4F9krwwyfwkD06ya3uvP0R3/+JDAJJsNfBcAkmabK+g6458ywjTlgN/2noMPbLNO2j49+KFwGPbd+uGdN2TB20CXF9VtyZ5Al2xMx0+AbwlyRbpHtb5NiZwv/0Y1gc2oMudd6R7WN2zB6b/HHhwkgWjLD8dxysfAl7VejAkyQPSPbBvE8Y+Fvs58Ae5933ay1n952Ii658Kf57kMUk2ortH/bPtivua7OuRcv9tdL3uNqI7fhivLwEPS/KGdA8u3CTJE9u0Y4B3txMmtM/qfhNoe51ngS5Nkar6JV03urdW1SXA++ge9PVzunuIz55Acy+hu4/7erri6O7ueVV1OfDndA/huo6uUH5eVf1+EjZjRK3t59M9EOY64GjgpVV12Sjz39ni2pXuCsZ1wHF0Z2Shuyfw4iS/oXtg3Iur6tbWle/dwNmtK9STJhDj5+iuxn+ydc26qMUL3UNXvgJcQdcF7Fbu3dXrM+33r5Kc316/le5s8A1098t/fC3WPxU+TvfQvh+2n3e1OE6ni/0kut4Y2wEvHqOtI4D/bfv8hXQnm+5P976dS9cVfVyq6kd0V6zeRPf5XU734CLoejH8ADi37aOv012pl6RJV1VXVtXSUSb/O939uz8H/pf7PgzrCAa+F6vqCrqC6Ot0T+8e/v+zXw38Y5Jf0xXJn2Z6vIvupP4KYCXdg73eNRkNV9Wv6R7w9Wm6XPgSul4CQ9MvoztB8MO2n7YctvyUH6+09/eVdA8yvYEuxyxp08Y6FjsDuBj4WZLr2rixPhfjXv8U+Shdj82fARvSHvq7hvv6n+hO7tyY5DC6Y82r6a66X0KX/8elfVae1db7M7q/kb3b5P+g+9yc1v4+zqU7xlWTqrXpQSpJmmlJVtE9xObrMx2LJEmaeknOpHsA33EzHYsml1fQJUmSJEnqAQt0SZIkSZJ6wC7ukiRJkiT1gFfQJUmSJEnqgfkzHYBmh80337wWLVo002FIkoBly5ZdV1VbzHQcml3M5ZLUH6Plcgt0jcuiRYtYunS0/0wiSZpOSa6e6Rg0+5jLJak/RsvldnGXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQemD/TAWh2WPmTm1h0+CkzHYZGsOrIfWc6BEmSJEmTwCvokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9MeYGe5GFJPpnkyiSXJPlykh2SXDTV654sSc5McnmSFUkuS/JfSTZbi/b+IMn/Jfl+2y//kWT9gemfaOt6e5LlA+MPSvLbJOu14Z2TrFiD9e+V5EtrGr8kaW4xl4/YnrlckjTpprRATxLgc8CZVbVdVT0GeDPw0MlqP8l09QI4uKp2AXYBbgP+b00aafvkZODzVbU9sAOwMfDuNv1hwJPbut4JbJtkk7b4k4HLgMcPDJ+9ZpsjSdLYzOX3ZS6XJE2VqU6IewO3V9UxQyOqajlwzdBwknlJ3pvkvHam+S/b+I2TnJ7k/CQrk+zXxi9KcmmSo4Hzga2TfDDJ0iQXJ3nHQNurkrxjoI0dB9o+vo1bkeQFbfyzk5zT5v9Mko2Hb1BV/R74W2CbJI9ry30+ybK2/kPbuFck+feBWF6Z5N+AZwK3VtXxrb07gb8BXp5kI+A04CHtbPtTgPOAJ7Zmdgc+QJfMab+/k+QBST7c9uEFA/tqxH07KMkebZlHjPVmSpLmJHP5PbGYyyVJU2qqC/SdgGVjzPMK4Kaq2gPYA3hlkocDtwIHVNVudAcH72tnrAEeBXykqh5fVVcD/1BVi+nOiD8jyS4D7V/X2vggcFgb99a2zp3b2e0zkmwOvAXYp82/FHjjSAG3RHwhsGMb9fKq2h1YDLw+yYOBTwLPT+vCBrwMOB547PB9UlU3Az8CHgk8H7iyqnatqm8B3wGenOQBwF3Amdw7qZ8N/ANwRtuHewPvbfOPtm8BSPJk4Bhgv6r64UjbKkma88zl5nJJ0jSZP9MBAM8GdklyYBteAGwP/Bh4T5Kn0yWzrbinO93VVXXuQBsvbGe75wMLgccAQ/dzndx+LwP+tL3eB3jx0MJVdUOS57blzm7HDusD56wm7gy8fn2SA9rrrYHtq+rcJGcAz01yKbBeVa1M8kygRmlvpPFnA28CvgWcV1VXJnlkki2Ajavqh0meTXcAMXTQsiGwDaPv298DjwaOBZ5dVdeOuIHdPj0UYN6mW6xmV0iS5jhz+T3t9TaXb7PNNqvZFZKkPpjqAv1i4MAx5gnwuqr66r1GJkuALYDdq+r2JKvokhXALQPzPZzubPoeLTmfMDAfdPeYAdzJPds7UgIN8LWqOmisjUoyD9gZuDTJXnQHCXtW1W+TnDmw/uPo7tO7jO6MO3T75AXD2tuU7mDgSuAhw1Z3Lt0Z86dyz0HGj+kOSr4zEPsLquryYe2Otm/3An7a4nw8MGJSr6pj6RI/GyzcfqQDDknSus9cvo7k8sWLF5vLJannprqL+xnABkleOTQiyR7AtgPzfBX4q9zzNNMdWpeuBcAvWkLfe9gygzalS/I3JXko8CfjiOs04LUDMT2QLnk+Jckj27iNkuwwfMEW5z8B11TVihbnDS2h7wg8aWjeqvouXbJ+CfCJNvp0YKMkL23tzQPeB5xQVb8dvr6q+jXdfX5LuCepnwO8gXuS+leB1w11G0zy+IHxI+1bgBuBfemubOy12r0lSZrLzOXmcknSNJnSAr2qCjgAeFa6f0FyMXAE9z7LexxwCXB+un/X8t90Z8dPBBYnWQocTHfmeqR1XAhcQHc2+8OM70mo7wIemOSiJBcCe1fVL+kS5yfS/buTc7nnvjSAE9v4i4AHAPu18acC89u0d7blBn0aOLuqbhi2T/4syfeBK+ju0XvzauI9G9igqoYeyHMO8AjuServBNYDVrR9+M42frR9S4vl58DzgA8kGXp4jSRJdzOXA+ZySdI0SZdjNFXS/Y/Sf6+q02c6lrWxwcLta+EhR810GBrBqiP3nekQJE2zJMvaA9U0DdaVXL548eJaunTpTIchSWL0XD5d/3d0zkmyWZIrgN/N9oQuSdJcZC6XJE23PjzFfZ1UVTcC97nvTZIkzQ7mcknSdPMKuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gA+J07jsvNUClvrvvCRJkiRpyngFXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQe8CnuGpeVP7mJRYefMtNhaJZZ5ZP/JUmSpHHzCrokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUA9NSoCc5IEkl2XGM+d6QZKOB4S8n2WwN1rdZklcPDG+Z5LMTbWdg+VVJVrafS5K8K8kGa9HeY5OckeSKJN9P8tYkadM2SPL1JMuTHJLk8wPL/X2SHwwMPy/JF9Zg/UuS/Neaxi9JmnvM5fdpz1wuSZp003UF/SDg28CLx5jvDcDdSb2qnlNVN67B+jYD7k7qVXVtVR24Bu0M2ruqdgaeADwCOHZNGklyf+ALwJFVtQPwOODJA/E+HlivqnYFvgzsObD4nsDNSR7Shp8MnL0mcUiSNEHm8sZcLkmaKlNeoCfZGHgK8ApaUk8yL8m/trPYK5K8LsnrgS2BbyT5RptvVZLNk/zzsLPoRyR5U5KNk5ye5PzW1n5tliOB7dqZ6/cmWZTkorbshkmOb/NfkGTvNn5JkpOTnNrOhP/LSNtTVb8BXgXsn+RBo8WQ5J1J/nog5ne3bXwJcHZVndba+y3wWuDwlqw/BuyaZDmwKXBTkke2ZrYCTqJL5rTf30myRZKTkpzXfp7S1vmAJB9u4y4Y2D+D78++Sc5Jsvm43lBJ0pxjLr87ZnO5JGlKzZ+GdewPnFpVVyS5PsluwBOBhwOPr6o7kjyoqq5P8ka6s9vXDWvjk8BRwNFt+IXAHwO3AgdU1c0tKZ2brpvY4cBO7cw1SRYNtPUagKraOV03vdOS7NCm7Up31vs24PIk76+qa4ZvUFvfVcD2wLJRYvgf4GTgP5Lcj+6A5gnAP7RlBtu7sh383Ar8BXBYVT23xf4d4MlJ5gHfB84F/ijJl4BdgPOADwP/XlXfTrIN8FXg0W1dZ1TVy9N1L/xekq8PrTfJAcAbgedU1Q3DtzPJocChAPM23WL4ZEnS3LE/5vJZn8u32Wab4ZMlST0zHQX6QXQJGbrkfBBdt7JjquoOgKq6fnUNVNUFSR6SZEtgC+CGqvpRkvWA9yR5OnAX3Vnph44Rz1OB97d2L0tyNTCU1E+vqpsAklwCbAvcJ6k3Gfh9nxiqalWSXyV5fIvpgqr6VZIANdqmjjDubLqz6/OAc4DvAW+jO/i4vKpuTbIP8JiuaQA2TbIJ8Gzg+UkOa+M3BIay897AYuDZVXXziMFUHUvr/rfBwu1Hi1mStO4zl68DuXzx4sXmcknquSkt0JM8GHgmsFOSoktMRXfWeaJJ4rPAgcDD6A4OAA6mS/K7V9XtSVbRJa7VhrWaabcNvL6TUfZPS5iLgCvGiOE4YEmL+cNt3MXA04e19wjgN1X164HEPOQ7wOvo9t2H2jwbAntxzz1r9wP2rKrfDWs3wAuq6vJh458I/JDu4GoHYOlI2ylJkrncXC5Jmj5TfQ/6gcBHqmrbqlpUVVsDVwHnA69KMh8gyYPa/L8GNhmlrU/SdS07kC7BAywAftGS6d50Z8nHaucsukRM6w63DXD5KPPeR+u+djTw+daVbLQYAD5H131vD7quagAnAk9tZ8qHHjTzn8CI98kBl9Ddz/c04II2bjndvXPfacOn0d37NhTjru3lV4HXteROuwIw5GrgT4GPJHnsODdfkjT3mMvN5ZKkaTLVBfpBdIlt0El0SepHwIokF9I9bAW6LlhfSXuwzKCqupguUf+kqn7aRp8ILE6ylC5RX9bm/RVwdpKLkrx3WFNHA/OSrAQ+BSypqtsY2zfSPZzmey32v1xdDC2O3wPfAD5dVXe2cb8D9gPekuRyYCXdvWcj/quUqirgu8B1VXV7G30O3RnzoaT++hbDitad71Vt/DuB9ej280VteLDty1vMn0my3Tj2gSRp7jGXm8slSdMkXc7QVGgPlDkf+LOq+v5Mx7M2Nli4fS085KiZDkOzzKoj953pEKR1UpJlVbV4puOYC9alXL548eJautSe8JLUB6Pl8un6P+hzTpLHAD+ge1jNrE7okiTNReZySdJ0m46nuM9JVXUJXdc1SZI0C5nLJUnTzSvokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPeA+6xmXnrRaw1CdyS5IkSdKU8Qq6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST3gU9w1Lit/chOLDj9lpsOQpF5b5X+7UI8Nz+V+XiWpf7yCLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPXAnCrQkzwsySeTXJnkkiRfTrJDkotmOrbxSnJmksUDw4tmU/ySJK0p87gkaV03f6YDmC5JAnwO+N+qenEbtyvw0ElsP1V112S0NxOSzKuqO2c6DkmShjOPj808Lkmz31y6gr43cHtVHTM0oqqWA9cMDSeZl+S9Sc5LsiLJX7bxGyc5Pcn5SVYm2a+NX5Tk0iRHA+cDWyf5YJKlSS5O8o6BtlclecdAGzsOtH18G7ciyQva+GcnOafN/5kkG4+1gUk2HGjrgiR7t/FLkvzXwHxfSrJXe/2bJP+Y5LvAnmu6cyVJmmLm8XvmM49L0jpqzlxBB3YClo0xzyuAm6pqjyQbAGcnOY0u+R9QVTcn2Rw4N8kX2jKPAl5WVa8GSPIPVXV9knnA6Ul2qaoVbd7rqmq3JK8GDgP+AnhrW+fObfkHtnW8Bdinqm5J8nfAG4F/bO2cmOR37fX6wNDZ/tcAVNXO7cDhtCQ7jLHNDwAuqqq3DZ+Q5FDgUIB5m24xRjOSJE0p8/jIRs3jLR5zuSTNInOpQB+PZwO7JDmwDS8Atgd+DLwnydPpkuhW3NOl7uqqOnegjRe2ZDgfWAg8BhhK7Ce338uAP22v9wFePLRwVd2Q5LltubO7HnesD5wzsI6Dq2opdGf/gS+18U8F3t/auSzJ1cBYif1O4KSRJlTVscCxABss3L7GaEeSpJlmHh/GXC5Js8tcKtAvBg4cY54Ar6uqr95rZLIE2ALYvapuT7IK2LBNvmVgvofTnVHfoyXoEwbmA7it/b6Te/Z9gOEJM8DXquqgsTfrPsuN5A7ufTvDYEy3er+aJGkWMI/fwzwuSeuouXQP+hnABkleOTQiyR7AtgPzfBX4qyTrtek7JHkA3Rn4X7SkvvewZQZtSpfob0ryUOBPxhHXacBrB2J6IHAu8JQkj2zjNhpHFzeAs4CDh2IHtgEuB1YBuya5X5KtgSeMoy1JkvrEPG4el6R13pwp0KuqgAOAZ6X79ywXA0cA1w7MdhxwCXB+un958t90Z8hPBBYnWUqXOC8bZR0XAhfQneX/MHD2OEJ7F/DAJBcluRDYu6p+CSwBPpFkBV2i33EcbR0NzEuyEvgUsKSqbmtxXAWsBP6V7kE4kiTNGuZx87gkzQXp8p20ehss3L4WHnLUTIchSb226sh9p2U9SZZV1eKx55TuMTyXT9fnVZJ0X6Pl8jlzBV2SJEmSpD6zQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpB+bPdACaHXbeagFL/XcskiTNWuZySeo/r6BLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg/4kDiNy8qf3MSiw0+Z6TAkadZb5UO6NEOGcrmfQUnqL6+gS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST0wpwr0JAckqSQ7jjHfG5JsNDD85SSbrcH6Nkvy6oHhLZN8dqLtDCy/KsnmA8N7JfnSmrYnSdJsYy6XJK3L5lSBDhwEfBt48RjzvQG4O6lX1XOq6sY1WN9mwN1JvaquraoD16CdKZXOXPssSJJmJ3P5CMzlkrRumDNf5Ek2Bp4CvIKW1JPMS/KvSVYmWZHkdUleD2wJfCPJN9p8q5JsnuSfh51FPyLJm5JsnOT0JOe3tvZrsxwJbJdkeZL3JlmU5KK27IZJjm/zX5Bk7zZ+SZKTk5ya5PtJ/mWc2/egJJ9v23Fukl0GYjxsYL6LWhyLklya5GjgfGDrtdrBkiRNMXP53fOZyyVpHTV/pgOYRvsDp1bVFUmuT7Ib8ETg4cDjq+qOJA+qquuTvBHYu6quG9bGJ4GjgKPb8AuBPwZuBQ6oqptbt7Vzk3wBOBzYqap2BUiyaKCt1wBU1c6tm95pSXZo03YFHg/cBlye5P1VdU2b9o0kd7bXGwOXtdfvAC6oqv2TPBP4SGtndR4FvKyqXj3GfJIk9cH+mMuHM5dL0jpkLhXoB9ElZOiS80HAI4BjquoOgKq6fnUNVNUFSR6SZEtgC+CGqvpRkvWA9yR5OnAXsBXw0DHieSrw/tbuZUmuBoaS+ulVdRNAkkuAbYGhpH73wUaSvYDDBtp7QWvvjCQPTrJgjBiurqpzR5uY5FDgUIB5m24xRlOSJE05c/l9mcslaR0yJwr0JA8GngnslKSAeUABy9rvifgscCDwMLqDA4CD6ZL87lV1e5JVwIZjhbWaabcNvL6T8b1PI7VXwB3c+1aGwbhuWV2DVXUscCzABgu3n+h+kiRp0pjLzeWSNBfMlXvQDwQ+UlXbVtWiqtoauIrufq1XJZkP3b1fbf5fA5uM0tYn6e57O5AuwQMsAH7REvredGfJx2rnLLqDAVp3uG2Ay9dw+4a3txdwXVXdDKwCdmvjd6PrBihJ0mxjLsdcLknrurlSoB8EfG7YuJPoHiDzI2BFkguBl7RpxwJfGXqwzKCqupguUf+kqn7aRp8ILE6ylC6xXtbm/RVwdnuYy3uHNXU0MC/JSuBTwJKquo01d0SLYQXdA20OGdjOByVZDvwVcMVarEOSpJliLjeXS9I6L1X2dtLYNli4fS085KiZDkOSZr1VR+671m0kWVZViychHM0hQ7l8Mj6DkqS1M1ounytX0CVJkiRJ6jULdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6YP5MB6DZYeetFrDUf8siSdKsZS6XpP7zCrokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPeBT3DUuK39yE4sOP2Wmw1gnrPIJupKkGTDeXG6ekqSZ4xV0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0OeoJGcmWTzTcUiSpIkzj0vSuskCXZIkSZKkHrBAnyWS/G2S17fX/57kjPb6D5N8LMmzk5yT5Pwkn0mycZu+e5JvJlmW5KtJFg5r935J/jfJu6Z/qyRJmhvM45Kk8bBAnz3OAp7WXi8GNk6yHvBUYCXwFmCfqtoNWAq8sU1/P3BgVe0OfBh490Cb84ETgSuq6i3DV5jk0CRLkyy987c3TdV2SZI0F0x7HgdzuSTNNvNnOgCN2zJg9ySbALcB59Ml+KcBXwAeA5ydBGB94BzgUcBOwNfa+HnATwfa/G/g01U1mOzvVlXHAscCbLBw+5r8TZIkac6Y9jwO5nJJmm0s0GeJqro9ySrgZcB3gBXA3sB2wFXA16rqoMFlkuwMXFxVe47S7HeAvZO8r6punbLgJUma48zjkqTxsIv77HIWcFj7/S3gVcBy4FzgKUkeCZBkoyQ7AJcDWyTZs41fL8ljB9r7H+DLwGeSeLJGkqSpZR6XJK2WBfrs8i1gIXBOVf0cuBX4VlX9ElgCfCLJCrpEv2NV/R44EPjnJBfSHQQ8ebDBqvo3um52H03i50GSpKljHpckrZZnW2eRqjodWG9geIeB12cAe4ywzHLg6SOM32vg9dsnOVRJkjSMeVySNBbPtEqSJEmS1AMW6JIkSZIk9YAFuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAz7FXeOy81YLWHrkvjMdhiRJWkPmcknqP6+gS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDPsVd47LyJzex6PBTJr3dVT5NVpKkabG6XG4+lqR+8Aq6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6HNAks2SvHpgeK8kX5rJmCRJ0viZyyVpbrBAnxs2A1491kySJKm3NsNcLknrPAv0nkmyKMllSY5LclGSE5Psk+TsJN9P8oQkD0ry+SQrkpybZJe27BFJPpzkzCQ/TPL61uyRwHZJlid5bxu3cZLPtnWdmCQzssGSJK1jzOWSpDU1f6YD0IgeCfwZcChwHvAS4KnA84E3A9cAF1TV/kmeCXwE2LUtuyOwN7AJcHmSDwKHAztV1a7QdYsDHg88FrgWOBt4CvDtwSCSHNpiYN6mW0zFdkqStK4yl0uSJswr6P10VVWtrKq7gIuB06uqgJXAIroE/1GAqjoDeHCSBW3ZU6rqtqq6DvgF8NBR1vG9qvpxW8fy1u69VNWxVbW4qhbP22jB8MmSJGl05nJJ0oRZoPfTbQOv7xoYvouu18NIXdhqhGXvZPReEuOdT5IkTZy5XJI0YRbos9NZwMFwdxe366rq5tXM/2u6bnKSJKkfzOWSpPvwTOvsdARwfJIVwG+BQ1Y3c1X9qj2Y5iLgK8ApUx+iJElajSMwl0uShkl3O5S0ehss3L4WHnLUpLe76sh9J71NSVrXJVlWVYtnOg7NLqvL5eZjSZpeo+Vyu7hLkiRJktQDFuiSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YD/B13jsvNWC1jqv2CRJGnWMpdLUv95BV2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesCHxGlcVv7kJhYdfsqEl1vlw2gkSeqFNc3lc4nHLZJmmlfQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJ9mSf4hycVJViRZnuSJU7iuVyV5aXu9JMmWU7UuSZLmCnO5JGmqzJ/pAOaSJHsCzwV2q6rbkmwOrD9F65pfVccMjFoCXARcOxXrkyRpLjCXS5KmklfQp9dC4Lqqug2gqq6rqmuT7J7km0mWJflqkoVJHp3ke0MLJlmUZEV7fZ/52/gzk7wnyTeBv05yRJLDkhwILAZObGf6903yuYG2n5Xk5OncEZIkzVLmcknSlLFAn16nAVsnuSLJ0UmekWQ94P3AgVW1O/Bh4N1VdSmwfpJHtGVfBHx6tPkH1rFZVT2jqt43NKKqPgssBQ6uql2BLwOPTrJFm+VlwPFTtdGSJK1DzOWSpCljF/dpVFW/SbI78DRgb+BTwLuAnYCvJQGYB/y0LfJp4IXAkXRJ/UXAo1YzP63NseKoJB8F/jzJ8cCewEuHz5fkUOBQgHmbbjF8siRJc465XJI0lSzQp1lV3QmcCZyZZCXwGuDiqtpzhNk/BXymdVmrqvp+kp1XMz/ALeMM5Xjgi8CtwGeq6o4RYj0WOBZgg4Xb1zjblSRpnWYulyRNFbu4T6Mkj0qy/cCoXYFLgS3aQ2dIsl6SxwJU1ZXAncBbueds+uWjzT+GXwObDA1U1bV0D5l5C3DCWmyWJElzhrlckjSVvII+vTYG3p9kM+AO4Ad03c6OBf4zyQK69+Qo4OK2zKeA9wIPB6iq37cHxYw2/2hOAI5J8jtgz6r6HXAisEVVXTJJ2ydJ0rrOXC5JmjKpsrfTXJXkv4ALqup/xpp3g4Xb18JDjprwOlYdue8aRCZJWp0ky6pq8UzHoZk3Hbl8LvG4RdJ0GS2XewV9jkqyjO4etzfNdCySJGnizOWStO6xQJ+j2r91kSRJs5S5XJLWPT4kTpIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wIfEaVx23moBS/3XI5IkzVrmcknqP6+gS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQDPsVd47LyJzex6PBTZjoMSROwyqc1SxpgLpekyTGVx1heQZckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJ9DSQ5IEkl2bENL0py0RSt64gkh01g/t9MZfuSJK0LzOWSpD6yQF8zBwHfBl68JgsnmTe54UiSpAkyl0uSescCfYKSbAw8BXgFIyT1dgb+W0nObz9PbuP3SvKNJB8HVrbhbyb5dJIrkhyZ5OAk30uyMsl2Y8Tx+STLklyc5NBh097X1n16ki3auO2SnNqW+dbQFQNJkuYac7kkqa8s0Cduf+DUqroCuD7JbsOm/wJ4VlXtBrwI+M+BaU8A/qGqHtOGHwf8NbAz8P8BO1TVE4DjgNeNEcfLq2p3YDHw+iQPbuMfAJzf1v9N4O1t/LHA69oyhwFHj7WhSQ5NsjTJ0jt/e9NYs0uSNFvsj7lcktRD82c6gFnoIOCo9vqTbfgDA9PXA/4rya7AncAOA9O+V1VXDQyfV1U/BUhyJXBaG78S2HuMOF6f5ID2emtge+BXwF3Ap9r4jwEntysFTwY+k2Ro+Q3GaJ+qOpbuYIANFm5fY80vSdIsYS6XJPWSBfoEtDPbzwR2SlLAPKC49xnsvwF+TndG/X7ArQPTbhnW5G0Dr+8aGL6L1bw3SfYC9gH2rKrfJjkT2HCU2avFcWNV7Tpam5IkzQXmcklSn9nFfWIOBD5SVdtW1aKq2hq4CviDgXkWAD+tqrvourpNxUNkFgA3tIS+I/CkgWn3a3ECvAT4dlXdDFyV5M8A0nncFMQlSVLfmcslSb1lgT4xBwGfGzbuJODNA8NHA4ckOZeuS9zwM+1r4i1Jfjz0A5wKzE+yAngncO7AvLcAj02yjO4KwT+28QcDr0hyIXAxsN8kxCVJ0mxjLpck9VaqvB1JY9tg4fa18JCjZjoMSROw6sh9ZzoETZEky6pq8UzHodnFXC5Jk2MyjrFGy+VeQZckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpB+bPdACaHXbeagFLfSK0JEmzlrlckvrPK+iSJEmSJPWABbokSZIkST1ggS5JkiRJUg9YoEuSJEmS1AMW6JIkSZIk9YBPcde4rPzJTSw6/JSZDmPGrPKpt5KkWW6u53JNL4+dpDXjFXRJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJ1GSA5JUkh0nsc39kzymvf5AkuVJLknyu/Z6eZIDJ2t9kiTNZeZySdJMskCfXAcB3wZePIlt7g88BqCqXlNVuwLPAa6sql3bz2cncX2SJM1l5nJJ0oyxQJ8kSTYGngK8gpbUkyxMclY7M35RkqclmZfkhDa8MsnftHm3S3JqkmVJvpVkxyRPBp4PvLe1sd0I6/1okv0Ghk9M8vwkS5L8X2vz8iRvH5jnz5N8r7X530nmTfHukSSp98zlkqSZNn+mA1iH7A+cWlVXJLk+yW7A3sBXq+rdLXFuBOwKbFVVOwEk2awtfyzwqqr6fpInAkdX1TOTfAH40mrOrB8H/A3wf0kWAE8GDgH+HHgCsBPwW+C8JKcAtwAvAp5SVbcnORo4GPjI8IaTHAocCjBv0y3WYtdIkjQr7I+5XJI0gyzQJ89BwFHt9Sfb8BeBDydZD/h8VS1P8kPgEUneD5wCnNbO2D8Z+EySofY2GM9Kq+qb7X62hwB/CpxUVXe0dr5WVb8CSHIy8FTgDmB3uiQPcH/gF6O0fSzdwQYbLNy+xrsjJEmapczlkqQZZYE+CZI8GHgmsFOSAuYBBfwt8HRgX+CjSd5bVR9J8jjgj4DXAC8E3gDc2O5JWxMfpTtz/mLg5QPjhyfiAgL8b1X9/RquS5KkdY65XJLUB96DPjkOBD5SVdtW1aKq2hq4ii6h/6KqPgT8D7Bbks2B+1XVScBbgd2q6mbgqiR/BpDO41rbvwY2GWP9J9AdGFBVFw+Mf1aSByW5P123vbOB04ED21l62vRt127zJUma9czlkqQZ5xX0yXEQcOSwcSfRJdtbktwO/AZ4KbAVcHySoZMjQ2e/DwY+mOQtwHp0XesubL8/lOT1wIFVdeXwlVfVz5NcCnx+2KRv052RfyTw8apaCtDWcVqL4Xa6s/9Xr9mmS5K0TjCXS5JmXKq8HWm2S7IRsJLuDP5NbdwSYHFVvXYy1rHBwu1r4SFHTUZTs9KqI/ed6RAk6W5JllXV4pmOQ5PHXK51jcdO0uqNlsvt4j7LJdkHuAx4/1BClyRJs4e5XJI0xC7us1xVfR3YZoTxJ9B1y5MkST1mLpckDfEKuiRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gA+J07jsvNUClvrvMiRJmrXM5ZLUf15BlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQe8CFxGpeVP7mJRYefMtNhaIas8qFCkjTrmcvnJnO4NLt4BV2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqgd4V6Em2TPLZNVhusySvXtt2pkuSJUm2nOZ1nplk8XSuU5I0t5jHp3Sd5nFJWsetdYGezqQV+lV1bVUduAaLbgbcndjXop3psgQYMbEnmTe9oUiS5irz+BpbgnlckjTJxpWQk7wxyUXt5w1JFiW5NMnRwPnA1knemuSyJF9L8okkh7VlX5nkvCQXJjkpyUZt/AlJ/jPJd5L8MMmBbfyiJBe118clWd5+fpnk7Uk2TnJ6kvOTrEyyXwvzSGC7Nu97h7WzYZLj2/wXJNm7jV+S5OQkpyb5fpJ/GdjmP27ruDDJ6W3cg5J8PsmKJOcm2aWNP2Joe9vwRW39Q/vpQ0kuTnJakvu3bV0MnNjivX+SVUneluTbwOFJzh9ob/sky9rrPdo+uzDJ95Jssprtu3+ST7Z4PwXcf6DNZyc5p23jZ5JsPN4PjSRpdjGPm8clSbPDmAV6kt2BlwFPBJ4EvBJ4IPAo4CNV9XhgC+AFwOOBP6VLWkNOrqo9qupxwKXAKwamLQSeCjyXLjHfS1X9RVXtCuwH/Ao4AbgVOKCqdgP2Bt6XJMDhwJVVtWtV/b9hTb2mtbczcBDwv0k2bNN2BV4E7Ay8KMnWSbYAPgS8oMX9Z23edwAXVNUuwJuBj4y1/4DtgQ9U1WOBG1ubnwWWAge3eH/X5r21qp5aVe8Gbkqyaxv/MuCEJOsDnwL+usW1D/C71WzfXwG/bfG+G9gdIMnmwFuAfdp+XAq8cRzbIkmaZczj5nFJ0uwxfxzzPBX4XFXdApDkZOBpwNVVde7APP83lKCSfHFg+Z2SvIuu69rGwFcHpn2+qu4CLkny0JFW3hLUZ4DXVtXVSdYD3pPk6cBdwFbAiMsO24b3A1TVZUmuBnZo006vqpvaui4BtqU7cDmrqq5qy1w/0M4L2rgzkjw4yYIx1n1VVS1vr5cBi1Yz76cGXh8HvCzJG+kOPJ5AdzD106o6r8Vwc4t7tO17OvCfbfyKJCta208CHgOc3R0TsT5wzvBgkhwKHAowb9MtxthMSVJPmcfnaB5vbZvLJWkWGU+BnlHG3zKOeaA7W75/VV2YZAmw18C028bRxjF0Z++/3oYPpjvTv3tV3Z5kFbDhKMuOJ77BGO6k2ycBapztFHAH9+6NMBjP8Pbvz+gG9+lJwNuBM4BlVfWrJFtNIK7B+Eaa/2tVddBqlqOqjgWOBdhg4fYjtSNJ6j/z+OrbWWfzOJjLJWm2Gc896GcB+yfZKMkDgAOAbw2b59vA89o9VBsD+w5M2wT4aTtjfvBEgkvyGmCTqhrsNrcA+EVL6nvTnSkH+HVb12jbcHBrcwdgG+Dy1az6HOAZSR7elnnQCO3sBVzXzn6vAnZr43cDHj6OzVtdvFTVrXRXKT4IHN9GXwZsmWSPtq5NksxfzfYNjt8J2KW1cy7wlCSPbNM2astJktY95nHzuCRplhjzCnpVnZ/kBOB7bdRxwA3D5jkvyReAC4Gr6e6FuqlNfivw3TZ+JatJZiM4DLg9yfI2fAxwIvDFJEuB5XTJjnZm+ux0D5T5CvCBgXaOBo5JspLuLPmSqrqtdQsbaZt/2bqEnZzuyba/AJ4FHAEc37qY/RY4pC1yEvDSFud5wBXj2LYTWky/A/YcZZ4T6e4FPK3F9fskLwLen+T+dPet7bOa7fvgQLzLae9h274lwCeSbNDW9ZZxxi1JmkXM4+ZxSdLskarJ6e2UZOOq+k26p7ueBRxaVeePtZxGl+6Jsguq6q0zHcsGC7evhYccNdNhaIasOnLfsWeSNG2SLKuqSf1/2ObxydenPA7m8rnKHC7102i5fDz3oI/XsUkeQ3ff1v+a1NdOks8B2wHPnOlYJElzgnl8EpnHJUlrYtIK9Kp6yWS1JaiqA2Y6BknS3GEen1zmcUnSmhjPQ+IkSZIkSdIUs0CXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknpgMp/irnXYzlstYKn/pkOSpFnLXC5J/ecVdEmSJEmSesACXZIkSZKkHrBAlyRJkiSpByzQJUmSJEnqAQt0SZIkSZJ6wAJdkiRJkqQesECXJEmSJKkHLNAlSZIkSeoBC3RJkiRJknrAAl2SJEmSpB6wQJckSZIkqQcs0CVJkiRJ6gELdEmSJEmSeiBVNdMxaBZI8mvg8pmOYxbYHLhupoPoOffR2NxHY5vr+2jbqtpipoPQ7DILcvls+Lvue4zGt3aMb+31PcY+xTdiLp8/E5FoVrq8qhbPdBB9l2Sp+2n13Edjcx+NzX0krZFe5/LZ8Hfd9xiNb+0Y39rre4x9jw/s4i5JkiRJUi9YoEuSJEmS1AMW6BqvY2c6gFnC/TQ299HY3Edjcx9JE9f3v5u+xwf9j9H41o7xrb2+x9j3+HxInCRJkiRJfeAVdEmSJEmSesACXZIkSZKkHrBAF0n+OMnlSX6Q5PARpifJf7bpK5LsNt5l1xVruY9WJVmZZHmSpdMb+fQZxz7aMck5SW5LcthEll1XrOU+8nPUTT+4/Y2tSPKdJI8b77LSXLE2Oasn8Y36XdiT+Eb9HupJfPu12JYnWZrkqdMZ33hiHJhvjyR3JjmwT/El2SvJTW0fLk/ytj7FNxDj8iQXJ/lmn+JL8v8G9t1F7T1+UM9iXJDki0kubPvwZdMZ32pVlT9z+AeYB1wJPAJYH7gQeMyweZ4DfAUI8CTgu+Nddl34WZt91KatAjaf6e3owT56CLAH8G7gsIksuy78rM0+8nN0r3meDDywvf6TufZ95I8/Y/2sbc7qSXyjfhf2JL4Rv4d6FN/G3POcqV2Ay/q2DwfmOwP4MnBgn+ID9gK+NJ37bYLxbQZcAmzThh/Sp/iGzf884Iwe7sM3A//cXm8BXA+sPxPv+fAfr6DrCcAPquqHVfV74JPAfsPm2Q/4SHXOBTZLsnCcy64L1mYfzRVj7qOq+kVVnQfcPtFl1xFrs4/mivHso+9U1Q1t8FzgD8a7rDRH9D1n9f27cG2+h/oS32+qVR3AA4DpfiL0eL+PXwecBPxiOoOj//liPPG9BDi5qn4E3d9Mz+IbdBDwiWmJ7B7jibGATZKE7qTW9cAd0xvmyCzQtRVwzcDwj9u48cwznmXXBWuzj6D7AjgtybIkh05ZlDNrbT4Lfo7Gx8/Rfb2C7irgmiwrravWNmdNtb7/ra7N99B0GFd8SQ5IchlwCvDyaYptyJgxJtkKOAA4ZhrjGjLe93jP1v35K0keOz2hAeOLbwfggUnObMcFL5226CbwN5JkI+CP6U7ETKfxxPhfwKOBa4GVwF9X1V3TE97qzZ/pADTjMsK44WdaR5tnPMuuC9ZmHwE8paquTfIQ4GtJLquqsyY1wpm3Np8FP0fj4+docMZkb7oD46F7K+fK50gay9rmrKnW97/Vtfkemg7jiq+qPgd8LsnTgXcC+0x1YAPGE+NRwN9V1Z3dBcxpNZ74zge2rarfJHkO8Hlg+6kOrBlPfPOB3YE/BO4PnJPk3Kq6YqqDY2J/w88Dzq6q66cwnpGMJ8Y/ApYDzwS2ozu2+lZV3TzFsY3JK+j6MbD1wPAf0J1JGs8841l2XbA2+4iqGvr9C+BzdN1u1jVr81nwczQOfo7ukWQX4Dhgv6r61USWleaAtcpZ06Dvf6tr8z00HSa0/9qJ3O2SbD7VgQ0YT4yLgU8mWQUcCBydZP9piW4c8VXVzVX1m/b6y8B607gPx/s3fGpV3VJV1wFnAdP1sMKJfAZfzPR3b4fxxfgyutsEqqp+AFwF7DhN8a2WBbrOA7ZP8vAk69P9IX1h2DxfAF7anvr6JOCmqvrpOJddF6zxPkrygCSbACR5APBs4KLpDH6arM1nwc/RGPwc3SPJNsDJwP837ErBXPkcSWNZm7zel/hm0tp8D/Ulvke2+2pJ94T+9YHpPIkwZoxV9fCqWlRVi4DPAq+uqs/3Jb4kDxvYh0+gq5mmax+O52/k/4CnJZnfupE/Ebi0R/GRZAHwjBbrdBtPjD+i64FAkocCjwJ+OK1RjsIu7nNcVd2R5LXAV+meePjhqro4yava9GPonq75HOAHwG/pzjiNuuwMbMaUWpt9BDyUrosZdH9vH6+qU6d5E6bcePZRkocBS4FNgbuSvIHuiZo3+zla/T4CNsfP0dDf2tuAB9NdbQG4o6oWz5XvI2ksa5mzehHf6vJFH+JjlO+hqY5tAvG9gO4EzO3A74AXDTw0ri8xzphxxncg8FdJ7qDbhy+ern04nviq6tIkpwIrgLuA46pqWk7cT+D9PQA4rapumY641iDGdwInJFlJ1yX+71pvhBmXafx7lSRJkiRJo7CLuyRJkiRJPWCBLkmSJElSD1igS5IkSZLUAxbokiRJkiT1gAW6JEmSJEk9YIEuSZIkSVIPWKBLkiRJktQD/z+eNxRKTrTK4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy import stats\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "# Select relevant columns\n",
    "selected_columns = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'ActivatedTimestamp', 'ClearedTimestamp', 'month', 'week', 'ResolutionTimeMinutes'\n",
    "]\n",
    "df_selected = df_sample[selected_columns]\n",
    "\n",
    "# Convert categorical columns to numerical using Label Encoding\n",
    "label_encoders = {}\n",
    "for column in ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity']:\n",
    "    le = LabelEncoder()\n",
    "    df_selected[column] = le.fit_transform(df_selected[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Convert timestamps to datetime and extract features\n",
    "df_selected['ActivatedTimestamp'] = pd.to_datetime(df_selected['ActivatedTimestamp'])\n",
    "df_selected['ClearedTimestamp'] = pd.to_datetime(df_selected['ClearedTimestamp'])\n",
    "df_selected['ActivationHour'] = df_selected['ActivatedTimestamp'].dt.hour\n",
    "df_selected['ClearanceHour'] = df_selected['ClearedTimestamp'].dt.hour\n",
    "df_selected['ActivationDayOfWeek'] = df_selected['ActivatedTimestamp'].dt.dayofweek\n",
    "df_selected['ClearanceDayOfWeek'] = df_selected['ClearedTimestamp'].dt.dayofweek\n",
    "df_selected['ResolutionTime'] = (df_selected['ClearedTimestamp'] - df_selected['ActivatedTimestamp']).dt.total_seconds() / 60\n",
    "\n",
    "# Remove rows with invalid resolution times\n",
    "df_selected = df_selected[(df_selected['ResolutionTime'] >= 0) & (df_selected['ResolutionTime'] <= 10000)]\n",
    "\n",
    "# Remove outliers\n",
    "z_scores = np.abs(stats.zscore(df_selected['ResolutionTimeMinutes']))\n",
    "df_selected = df_selected[z_scores < 3]\n",
    "\n",
    "# Normalize the ResolutionTimeMinutes column\n",
    "scaler = MinMaxScaler()\n",
    "df_selected['ResolutionTimeMinutes'] = scaler.fit_transform(df_selected[['ResolutionTimeMinutes']])\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    'organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity',\n",
    "    'month', 'week', 'ActivationHour', 'ClearanceHour',\n",
    "    'ActivationDayOfWeek', 'ClearanceDayOfWeek'\n",
    "]\n",
    "X = df_selected[features]\n",
    "y = df_selected['ResolutionTimeMinutes']\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Feature importance using Random Forest\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(X_train, y_train)\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# Mutual Information for feature importance\n",
    "mi = mutual_info_regression(X_train, y_train)\n",
    "mi_importances = mi\n",
    "\n",
    "# Plot feature importances\n",
    "feature_names = X_train.columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "axs[0].barh(feature_names, rf_importances)\n",
    "axs[0].set_title('Random Forest Feature Importance')\n",
    "\n",
    "# Mutual Information Feature Importance\n",
    "axs[1].barh(feature_names, mi_importances)\n",
    "axs[1].set_title('Mutual Information Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Remove less important columns\n",
    "important_features = ['organizationcountrycode', 'AssetType', 'AlarmLabel', 'Severity', 'month', 'week', 'ActivationHour', 'ClearanceHour']\n",
    "X_train = X_train[important_features]\n",
    "X_val = X_val[important_features]\n",
    "X_test = X_test[important_features]\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    hidden_layer_sizes = trial.suggest_categorical('hidden_layer_sizes', [(100,), (100, 50), (150, 75), (200, 100)])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'tanh'])\n",
    "    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])\n",
    "    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n",
    "    learning_rate = trial.suggest_categorical('learning_rate', ['constant', 'adaptive'])\n",
    "    max_iter = trial.suggest_int('max_iter', 300, 500)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=hidden_layer_sizes,\n",
    "        activation=activation,\n",
    "        solver=solver,\n",
    "        alpha=alpha,\n",
    "        learning_rate=learning_rate,\n",
    "        max_iter=max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b93710b-a83c-4122-8f0c-d9214fb0a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another version for the above code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4a9294c-1c8f-4be2-9d39-2044e92a0178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AssetId</th>\n",
       "      <th>organizationid</th>\n",
       "      <th>organizationcountrycode</th>\n",
       "      <th>locationid</th>\n",
       "      <th>AssetType</th>\n",
       "      <th>AlarmLabel</th>\n",
       "      <th>AlarmMessage</th>\n",
       "      <th>Severity</th>\n",
       "      <th>ActivatedTimestamp</th>\n",
       "      <th>ClearedTimestamp</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>ResolutionTimeMinutes</th>\n",
       "      <th>Lemmas_No_Stop_Words</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34706487</th>\n",
       "      <td>6255f030-8744-4d1e-b7ca-f6f3e4e4774d</td>\n",
       "      <td>c7553c18-7a3d-4728-8d19-b10267ae72e5</td>\n",
       "      <td>US</td>\n",
       "      <td>2ceb1e29-6c3a-44f3-8ab0-4f40e54b870e</td>\n",
       "      <td>UPS</td>\n",
       "      <td>Battery Charger Fault</td>\n",
       "      <td>A battery charger error exists.</td>\n",
       "      <td>WARNING</td>\n",
       "      <td>2021-06-16T00:23:55Z</td>\n",
       "      <td>2021-06-16T00:24:26Z</td>\n",
       "      <td>6.0</td>\n",
       "      <td>24</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>['battery', 'charger', 'fault']</td>\n",
       "      <td>['battery', 'charger', 'fault']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       AssetId  \\\n",
       "34706487  6255f030-8744-4d1e-b7ca-f6f3e4e4774d   \n",
       "\n",
       "                                organizationid organizationcountrycode  \\\n",
       "34706487  c7553c18-7a3d-4728-8d19-b10267ae72e5                      US   \n",
       "\n",
       "                                    locationid AssetType  \\\n",
       "34706487  2ceb1e29-6c3a-44f3-8ab0-4f40e54b870e       UPS   \n",
       "\n",
       "                     AlarmLabel                     AlarmMessage Severity  \\\n",
       "34706487  Battery Charger Fault  A battery charger error exists.  WARNING   \n",
       "\n",
       "            ActivatedTimestamp      ClearedTimestamp  month  week  \\\n",
       "34706487  2021-06-16T00:23:55Z  2021-06-16T00:24:26Z    6.0    24   \n",
       "\n",
       "          ResolutionTimeMinutes             Lemmas_No_Stop_Words  \\\n",
       "34706487               0.516667  ['battery', 'charger', 'fault']   \n",
       "\n",
       "                                   Tokens  \n",
       "34706487  ['battery', 'charger', 'fault']  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =df.sample(n=1,random_state = 106)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7e58889-c018-452e-b1aa-bd4359d36122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34706487    2021-06-16T00:23:55Z\n",
       "Name: ActivatedTimestamp, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ActivatedTimestamp.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e998267-f159-4876-8400-388dad8a4132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff2bd9b-6936-444a-a6d3-cf86a7cac00e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135d59e-a36c-4f15-b01b-3d7a0ceec351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a032462-96c8-4a27-b330-dd1cc1f76eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39f62d-17a6-4f76-9771-ef3b121b83c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa35338-fa6e-41b4-8c10-e22c1f2750c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b73657-635c-4c77-aa4c-ab9ebe6d7b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad65464c-046b-4edb-993d-e2fad9d33b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "499fc358-f785-4dab-85d9-1b1a17178df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'poly'}\n",
      "Validation MSE: 1717.1306760280086\n",
      "Test MSE: 12844.168962247342\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Further downsample the data to 100,000 rows for this example\n",
    "df_sample = df.sample(n=1000, random_state=42, replace=True)  # Adjust this as needed\n",
    "\n",
    "# Check for missing values and handle them if necessary\n",
    "df_sample = df_sample.dropna()  # You can also fillna() with some value if appropriate\n",
    "\n",
    "# Encode categorical variables\n",
    "df_encoded_sample = pd.get_dummies(df_sample, columns=['organizationcountrycode', 'Severity', 'AssetType'])\n",
    "\n",
    "# Drop all other variables except AssetType, organizationcountrycode, and Severity\n",
    "columns_to_keep = [col for col in df_encoded_sample.columns if col.startswith('organizationcountrycode') or col.startswith('Severity') or col.startswith('AssetType')]\n",
    "X_sample = df_encoded_sample[columns_to_keep]\n",
    "\n",
    "# Define the target variable\n",
    "y_sample = df_encoded_sample['ResolutionTimeMinutes']\n",
    "\n",
    "# Split the downsampled data\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_sample, y_sample, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Initialize the model\n",
    "svr = SVR()\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_svr = GridSearchCV(estimator=svr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "grid_search_svr.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svr = grid_search_svr.best_params_\n",
    "print(f\"Best parameters: {best_params_svr}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model_svr = grid_search_svr.best_estimator_\n",
    "best_model_svr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_svr = best_model_svr.predict(X_val)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on validation set\n",
    "mse_val_svr = mean_squared_error(y_val, y_val_pred_svr)\n",
    "print(f\"Validation MSE: {mse_val_svr}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_svr = best_model_svr.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on test set\n",
    "mse_test_svr = mean_squared_error(y_test, y_test_pred_svr)\n",
    "print(f\"Test MSE: {mse_test_svr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8d0749a-a0b0-4fbf-a760-433d44b4d6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.1s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=0.1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ....................C=10, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.3s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.1s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ....................C=1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=auto, kernel=rbf; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.3s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END ..................C=0.1, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END .................C=0.1, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...................C=1, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=10, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END ..................C=10, gamma=scale, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END ....................C=0.1, gamma=scale, kernel=poly; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END ..................C=0.1, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END .....................C=1, gamma=auto, kernel=linear; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=scale, kernel=linear; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=scale, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=auto, kernel=poly; total time=   0.0s\n",
      "[CV] END ...................C=10, gamma=auto, kernel=sigmoid; total time=   0.0s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "Best parameters: {'colsample_bytree': 0.6, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "Validation MSE: 2922.1438085444533\n",
      "Test MSE: 12110.023267890254\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Initialize the model\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgbr, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "print(f\"Best parameters: {best_params_xgb}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model_xgb = grid_search_xgb.best_estimator_\n",
    "best_model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_xgb = best_model_xgb.predict(X_val)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on validation set\n",
    "mse_val_xgb = mean_squared_error(y_val, y_val_pred_xgb)\n",
    "print(f\"Validation MSE: {mse_val_xgb}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_xgb = best_model_xgb.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on test set\n",
    "mse_test_xgb = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "print(f\"Test MSE: {mse_test_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38b603fc-ee2d-47fa-a312-0c9cf2684be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement skopt (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for skopt\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.6, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=5, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=0.8, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=5, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.01, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=3, n_estimators=300, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=5, n_estimators=300, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.6; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.8; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=1.0; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=3, n_estimators=200, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=100, subsample=1.0; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=5, n_estimators=200, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=100, subsample=0.8; total time=   0.1s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.6; total time=   0.2s\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=300, subsample=0.8; total time=   0.2s\n"
     ]
    }
   ],
   "source": [
    "pip install skopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fe9b2fc-b3d8-49be-b4ef-40a9120a9013",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskopt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BayesSearchCV\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m xgbr \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skopt'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# Initialize the model\n",
    "xgbr = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Define the parameter space for hyperparameter tuning using Bayesian Optimization\n",
    "param_space = {\n",
    "    'n_estimators': (100, 300),\n",
    "    'learning_rate': (0.01, 0.1, 'log-uniform'),\n",
    "    'max_depth': (3, 7),\n",
    "    'subsample': (0.6, 1.0),\n",
    "    'colsample_bytree': (0.6, 1.0)\n",
    "}\n",
    "\n",
    "# Initialize BayesSearchCV\n",
    "bayes_search_xgb = BayesSearchCV(estimator=xgbr, search_spaces=param_space, cv=3, n_jobs=-1, verbose=2, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model\n",
    "bayes_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb_bayes = bayes_search_xgb.best_params_\n",
    "print(f\"Best parameters: {best_params_xgb_bayes}\")\n",
    "\n",
    "# Train the model with the best parameters\n",
    "best_model_xgb_bayes = bayes_search_xgb.best_estimator_\n",
    "best_model_xgb_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred_xgb_bayes = best_model_xgb_bayes.predict(X_val)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on validation set\n",
    "mse_val_xgb_bayes = mean_squared_error(y_val, y_val_pred_xgb_bayes)\n",
    "print(f\"Validation MSE: {mse_val_xgb_bayes}\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_xgb_bayes = best_model_xgb_bayes.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error on test set\n",
    "mse_test_xgb_bayes = mean_squared_error(y_test, y_test_pred_xgb_bayes)\n",
    "print(f\"Test MSE: {mse_test_xgb_bayes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
